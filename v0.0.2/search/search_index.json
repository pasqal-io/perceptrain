{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/accelerator/","title":"Accelerator","text":""},{"location":"api/accelerator/#accelerator","title":"Accelerator","text":""},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator","title":"<code>Accelerator(nprocs=1, compute_setup='auto', log_setup='cpu', backend='gloo', dtype=None)</code>","text":"<p>               Bases: <code>Distributor</code></p> <p>A class for handling distributed training.</p> <p>This class extends <code>Distributor</code> to manage distributed training using PyTorch's <code>torch.distributed</code> API. It supports spawning multiple processes and wrapping models with <code>DistributedDataParallel</code> (DDP) when required.</p> <p>This class is provides head level method - distribute() - which wraps a function at a head process level, before launching <code>nprocs</code> processes as required. Furthermore, it provides processes level methods, such as prepare(), and prepare_batch() which can be run inside each process for correct movement and preparation of model, optimizers and datasets.</p> Inherited Attributes <p>nprocs (int): Number of processes to launch for distributed training. execution (BaseExecution): Detected execution instance for process launch (e.g., \"torchrun\",\"default\"). execution_type (ExecutionType): Type of execution used. rank (int): Global rank of the process (to be set during environment setup). world_size (int): Total number of processes (to be set during environment setup). local_rank (int | None): Local rank on the node (to be set during environment setup). master_addr (str): Master node address (to be set during environment setup). master_port (str): Master node port (to be set during environment setup). node_rank (int): Rank of the node on the cluster setup.</p> There are three different indicators for number of processes executed. <ul> <li> <ol> <li>self._config_nprocs: Number of processes specified by the user. Provided in the initilization of the Accelerator. (acc = Accelerator(nprocs = 2))</li> </ol> </li> <li> <ol> <li>self.nprocs: Number of processes defined at the head level.</li> <li>When accelerator is used to spawn processes (e.g., In case default, python execution), nprocs = _config_nprocs.</li> <li>When an external elastic method is used to spawn processes (e.g., In case of torchrun), nprocs = 1. This is because the external launcher already spawns multiple processes, and the accelerator init is called from each process.</li> </ol> </li> <li> <ol> <li>self.world_size: Number of processes actually executed.</li> </ol> </li> </ul> <p>Initializes the Accelerator class.</p> PARAMETER DESCRIPTION <code>nprocs</code> <p>Number of processes to launch. Default is 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>compute_setup</code> <p>Compute device setup; options are \"auto\" (default), \"gpu\", or \"cpu\". - \"auto\": Uses GPU if available, otherwise CPU. - \"gpu\": Forces GPU usage, raising an error if no CUDA device is available. - \"cpu\": Forces CPU usage.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'auto'</code> </p> <code>log_setup</code> <p>Logging device setup; options are \"auto\", \"cpu\" (default). - \"auto\": Uses same device to log as used for computation. - \"cpu\": Forces CPU logging.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cpu'</code> </p> <code>backend</code> <p>The backend for distributed communication. Default is \"gloo\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'gloo'</code> </p> <code>dtype</code> <p>Data type for controlling numerical precision. Default is None.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def __init__(\n    self,\n    nprocs: int = 1,\n    compute_setup: str = \"auto\",\n    log_setup: str = \"cpu\",\n    backend: str = \"gloo\",\n    dtype: torch_dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the Accelerator class.\n\n    Args:\n        nprocs (int): Number of processes to launch. Default is 1.\n        compute_setup (str): Compute device setup; options are \"auto\" (default), \"gpu\", or \"cpu\".\n            - \"auto\": Uses GPU if available, otherwise CPU.\n            - \"gpu\": Forces GPU usage, raising an error if no CUDA device is available.\n            - \"cpu\": Forces CPU usage.\n        log_setup (str): Logging device setup; options are \"auto\", \"cpu\" (default).\n            - \"auto\": Uses same device to log as used for computation.\n            - \"cpu\": Forces CPU logging.\n        backend (str): The backend for distributed communication. Default is \"gloo\".\n        dtype (torch.dtype | None): Data type for controlling numerical precision. Default is None.\n    \"\"\"\n    super().__init__(nprocs, compute_setup, log_setup, backend, dtype)\n\n    # Default values\n    self.rank = 0\n    self.local_rank = 0\n    self.world_size = self.execution.get_world_size(0, self.nprocs)\n</code></pre>"},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator.all_reduce_dict","title":"<code>all_reduce_dict(d, op='mean')</code>","text":"<p>Performs an all-reduce operation on a dictionary of tensors, averaging values across all processes.</p> PARAMETER DESCRIPTION <code>d</code> <p>A dictionary where values are tensors to be reduced across processes.</p> <p> TYPE: <code>dict[str, Tensor]</code> </p> <code>op</code> <p>Operation method to all_reduce with. Available options include <code>sum</code>, <code>avg</code>, and <code>max</code>.             Defaults to <code>avg</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'mean'</code> </p> RETURNS DESCRIPTION <code>dict[str, Tensor]</code> <p>dict[str, torch.Tensor]: A dictionary with the reduced tensors, averaged over the world size.</p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def all_reduce_dict(\n    self, d: dict[str, torch.Tensor], op: str = \"mean\"\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Performs an all-reduce operation on a dictionary of tensors, averaging values across all processes.\n\n    Args:\n        d (dict[str, torch.Tensor]): A dictionary where values are tensors to be reduced across processes.\n        op (str): Operation method to all_reduce with. Available options include `sum`, `avg`, and `max`.\n                        Defaults to `avg`\n\n    Returns:\n        dict[str, torch.Tensor]: A dictionary with the reduced tensors, averaged over the world size.\n    \"\"\"\n    if dist.is_initialized():\n        world_size = dist.get_world_size()\n        reduced: dict[str, torch.Tensor] = {}\n        for key, tensor in d.items():\n            if not isinstance(tensor, torch.Tensor):\n                tensor = torch.tensor(\n                    tensor, device=self.execution.device, dtype=self.execution.data_dtype\n                )\n            tensor = tensor.detach().clone()\n            if op == \"max\":\n                dist.all_reduce(tensor, op=dist.ReduceOp.MAX)\n            elif op == \"sum\":\n                dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n            else:\n                dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n                tensor /= world_size\n            reduced[key] = tensor\n        return reduced\n    else:\n        return d\n</code></pre>"},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator.broadcast","title":"<code>broadcast(obj, src)</code>","text":"<p>Broadcasts an object from the source process to all processes.</p> <p>On non-source processes, this value is ignored.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to broadcast on the source process.</p> <p> TYPE: <code>Any</code> </p> <code>src</code> <p>The source process rank.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The broadcasted object from the source process.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def broadcast(self, obj: Any, src: int) -&gt; Any:\n    \"\"\"\n    Broadcasts an object from the source process to all processes.\n\n    On non-source processes, this value is ignored.\n\n    Args:\n        obj (Any): The object to broadcast on the source process.\n        src (int): The source process rank.\n\n    Returns:\n        Any : The broadcasted object from the source process.\n    \"\"\"\n    if dist.is_initialized():\n        obj_list = [obj] if self.rank == src else [None]\n        dist.broadcast_object_list(obj_list, src=src)\n        return obj_list[0]\n    else:\n        return obj\n</code></pre>"},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator.distribute","title":"<code>distribute(fun)</code>","text":"<p>Decorator to distribute the fit function across multiple processes.</p> <p>This function is generic and can work with other methods as well. Weather it is bound or unbound.</p> <p>When applied to a function (typically a fit function), this decorator will execute the function in a distributed fashion using torch.multiprocessing. The number of processes used is determined by <code>self.nprocs</code>, and if multiple nodes are involved (<code>self.num_nodes &gt; 1</code>), the process count is adjusted accordingly. In single process mode (<code>self.nporcs</code> is 1), the function is executed directly in the current process.</p> <p>After execution, the decorator returns the model stored in <code>instance.model</code>.</p> PARAMETER DESCRIPTION <code>fun</code> <p>The function to be decorated. This function usually implements             a model fitting or training routine.</p> <p> TYPE: <code>callable</code> </p> RETURNS DESCRIPTION <code>callable</code> <p>The wrapped function. When called, it will execute in distributed mode       (if configured) and return the value of <code>instance.model</code>.</p> <p> TYPE: <code>Callable</code> </p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def distribute(self, fun: Callable) -&gt; Callable:\n    \"\"\"\n    Decorator to distribute the fit function across multiple processes.\n\n    This function is generic and can work with other methods as well.\n    Weather it is bound or unbound.\n\n    When applied to a function (typically a fit function), this decorator\n    will execute the function in a distributed fashion using torch.multiprocessing.\n    The number of processes used is determined by `self.nprocs`,\n    and if multiple nodes are involved (`self.num_nodes &gt; 1`), the process count is\n    adjusted accordingly. In single process mode (`self.nporcs` is 1), the function\n    is executed directly in the current process.\n\n    After execution, the decorator returns the model stored in `instance.model`.\n\n    Parameters:\n        fun (callable): The function to be decorated. This function usually implements\n                        a model fitting or training routine.\n\n    Returns:\n        callable: The wrapped function. When called, it will execute in distributed mode\n                  (if configured) and return the value of `instance.model`.\n    \"\"\"\n\n    @functools.wraps(fun)\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n\n        # Get the original picklable function\n        # for the case of bound class method\n        # as well as a function\n        if self.is_class_method(fun, args):\n            instance = args[0]\n            method_name = fun.__name__\n            method = getattr(instance, method_name)\n            args = args[1:]\n            self._spawn_method(instance, method, args, kwargs)\n        else:\n            instance = None\n            # method_name = fun.__name__\n            # module = inspect.getmodule(fun)\n            # method = getattr(module, method_name) if module else fun\n            self._spawn_method(instance, fun, args, kwargs)\n\n        if instance and hasattr(instance, \"accelerator\"):\n            instance.accelerator.finalize()\n        else:\n            self.finalize()\n\n        # TODO: Return the original returns from fun\n        # Currently it only returns the model and optimizer\n        # similar to the fit method.\n        try:\n            return instance.model, instance.optimizer\n        except Exception:\n            return\n\n    return wrapper\n</code></pre>"},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator.is_class_method","title":"<code>is_class_method(fun, args)</code>","text":"<p>Determines if <code>fun</code> is a class method or a standalone function.</p> <p>Frist argument of the args should be: - An object and has dict: making it a class - Has a method named fun: making it a class that has this method.</p> PARAMETER DESCRIPTION <code>fun</code> <p>The function being checked.</p> <p> TYPE: <code>Callable</code> </p> <code>args</code> <p>The arguments passed to the function.</p> <p> TYPE: <code>tuple</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if <code>fun</code> is a class method, False otherwise.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def is_class_method(self, fun: Callable, args: Any) -&gt; bool:\n    \"\"\"\n    Determines if `fun` is a class method or a standalone function.\n\n    Frist argument of the args should be:\n    - An object and has __dict__: making it a class\n    - Has a method named fun: making it a class that has this method.\n\n    Args:\n        fun (Callable): The function being checked.\n        args (tuple): The arguments passed to the function.\n\n    Returns:\n        bool: True if `fun` is a class method, False otherwise.\n    \"\"\"\n    return (\n        bool(args)\n        and isinstance(args[0], object)\n        and hasattr(args[0], \"__dict__\")\n        and hasattr(args[0], fun.__name__)\n    )\n</code></pre>"},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator.prepare","title":"<code>prepare(*args)</code>","text":"<p>Prepares models, optimizers, and dataloaders for distributed training.</p> <p>This method iterates over the provided objects and: - Moves models to the specified device (e.g., GPU or CPU) and casts them to the     desired precision (specified by <code>self.dtype</code>). It then wraps models in     DistributedDataParallel (DDP) if more than one device is used. - Passes through optimizers unchanged. - For dataloaders, it adjusts them to use a distributed sampler (if applicable)     by calling a helper method. Note that only the sampler is prepared; moving the     actual batch data to the device is handled separately during training.     Please use the <code>prepare_batch</code> method to move the batch to correct device/dtype.</p> PARAMETER DESCRIPTION <code>*args</code> <p>A variable number of objects to be prepared. These can include: - PyTorch models (<code>nn.Module</code>) - Optimizers (<code>optim.Optimizer</code>) - DataLoaders (or a dictionary-like <code>DictDataLoader</code> of dataloaders)</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>tuple[Any, ...]</code> <p>tuple[Any, ...]: A tuple containing the prepared objects, where each object has been             modified as needed to support distributed training.</p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def prepare(self, *args: Any) -&gt; tuple[Any, ...]:\n    \"\"\"\n    Prepares models, optimizers, and dataloaders for distributed training.\n\n    This method iterates over the provided objects and:\n    - Moves models to the specified device (e.g., GPU or CPU) and casts them to the\n        desired precision (specified by `self.dtype`). It then wraps models in\n        DistributedDataParallel (DDP) if more than one device is used.\n    - Passes through optimizers unchanged.\n    - For dataloaders, it adjusts them to use a distributed sampler (if applicable)\n        by calling a helper method. Note that only the sampler is prepared; moving the\n        actual batch data to the device is handled separately during training.\n        Please use the `prepare_batch` method to move the batch to correct device/dtype.\n\n    Args:\n        *args (Any): A variable number of objects to be prepared. These can include:\n            - PyTorch models (`nn.Module`)\n            - Optimizers (`optim.Optimizer`)\n            - DataLoaders (or a dictionary-like `DictDataLoader` of dataloaders)\n\n    Returns:\n        tuple[Any, ...]: A tuple containing the prepared objects, where each object has been\n                        modified as needed to support distributed training.\n    \"\"\"\n    prepared: list = []\n    for obj in args:\n        if obj is None:\n            prepared.append(None)\n        elif isinstance(obj, nn.Module):\n            prepared.append(self._prepare_model(obj))\n        elif isinstance(obj, optim.Optimizer):\n            prepared.append(self._prepare_optimizer(obj))\n        elif isinstance(obj, (DataLoader, DictDataLoader)):\n            prepared.append(self._prepare_data(obj))\n        else:\n            prepared.append(obj)\n    return tuple(prepared)\n</code></pre>"},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator.prepare_batch","title":"<code>prepare_batch(batch)</code>","text":"<p>Moves a batch of data to the target device and casts it to the desired data dtype.</p> <p>This method is typically called within the optimization step of your training loop. It supports various batch formats:     - If the batch is a dictionary, each value is moved individually.     - If the batch is a tuple or list, each element is processed and returned as a tuple.     - Otherwise, the batch is processed directly.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data to move to the device. This can be a dict, tuple, list,          or any type compatible with <code>data_to_device</code>.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The batch with all elements moved to <code>self.device</code> and cast to <code>self.data_dtype</code>.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def prepare_batch(self, batch: dict | list | tuple | torch.Tensor | None) -&gt; Any:\n    \"\"\"\n    Moves a batch of data to the target device and casts it to the desired data dtype.\n\n    This method is typically called within the optimization step of your training loop.\n    It supports various batch formats:\n        - If the batch is a dictionary, each value is moved individually.\n        - If the batch is a tuple or list, each element is processed and returned as a tuple.\n        - Otherwise, the batch is processed directly.\n\n    Args:\n        batch (Any): The batch of data to move to the device. This can be a dict, tuple, list,\n                     or any type compatible with `data_to_device`.\n\n    Returns:\n        Any: The batch with all elements moved to `self.device` and cast to `self.data_dtype`.\n    \"\"\"\n    if batch is None:\n        return None\n\n    if isinstance(batch, dict):\n        return {\n            key: data_to_device(\n                value, device=self.execution.device, dtype=self.execution.data_dtype\n            )\n            for key, value in batch.items()\n        }\n    elif isinstance(batch, (tuple, list)):\n        return tuple(\n            data_to_device(x, device=self.execution.device, dtype=self.execution.data_dtype)\n            for x in batch\n        )\n    elif isinstance(batch, torch.Tensor):\n        return data_to_device(\n            batch, device=self.execution.device, dtype=self.execution.data_dtype\n        )\n    return\n</code></pre>"},{"location":"api/accelerator/#perceptrain.train_utils.accelerator.Accelerator.worker","title":"<code>worker(rank, instance, fun, args, kwargs)</code>","text":"<p>Worker function to be executed in each spawned process.</p> <p>This function is called in every subprocess created by torch.multiprocessing (via mp.spawn). It performs the following tasks:   1. Sets up the accelerator for the given process rank. This typically involves configuring      the GPU or other hardware resources for distributed training.   2. If the retrieved method has been decorated (i.e. it has a 'wrapped' attribute),      the original, unwrapped function is invoked with the given arguments. Otherwise,      the method is called directly.</p> PARAMETER DESCRIPTION <code>rank</code> <p>The rank (or identifier) of the spawned process.</p> <p> TYPE: <code>int</code> </p> <code>instance</code> <p>The object (Trainer) that contains the method to execute.                This object is expected to have an <code>accelerator</code> attribute with a <code>setup_process(rank)</code> method.                This argument is optional, in case it is None, the fun will be called independently.</p> <p> TYPE: <code>object</code> </p> <code>fun</code> <p>The function of the method on the instance to be executed.</p> <p> TYPE: <code>Callable</code> </p> <code>args</code> <p>Positional arguments to pass to the target method.</p> <p> TYPE: <code>tuple</code> </p> <code>kwargs</code> <p>Keyword arguments to pass to the target method.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>perceptrain/train_utils/accelerator.py</code> <pre><code>def worker(self, rank: int, instance: Any, fun: Callable, args: tuple, kwargs: dict) -&gt; None:\n    \"\"\"\n    Worker function to be executed in each spawned process.\n\n    This function is called in every subprocess created by torch.multiprocessing (via mp.spawn).\n    It performs the following tasks:\n      1. Sets up the accelerator for the given process rank. This typically involves configuring\n         the GPU or other hardware resources for distributed training.\n      2. If the retrieved method has been decorated (i.e. it has a '__wrapped__' attribute),\n         the original, unwrapped function is invoked with the given arguments. Otherwise,\n         the method is called directly.\n\n    Args:\n        rank (int): The rank (or identifier) of the spawned process.\n        instance (object): The object (Trainer) that contains the method to execute.\n                           This object is expected to have an `accelerator` attribute with a `setup_process(rank)` method.\n                           This argument is optional, in case it is None, the fun will be called independently.\n        fun (Callable): The function of the method on the instance to be executed.\n        args (tuple): Positional arguments to pass to the target method.\n        kwargs (dict): Keyword arguments to pass to the target method.\n    \"\"\"\n    # Setup the accelerator for the given process rank (e.g., configuring GPU)\n    if instance and instance.accelerator:\n        instance.accelerator.setup_process(rank)\n    else:\n        self.setup_process(rank)\n\n    if hasattr(fun, \"__wrapped__\"):\n        # Explicitly get the original (unbound) method, passing in the instance.\n        # We need to call the original method in case so that MP spawn does not\n        # create multiple processes. (To Avoid infinite loop)\n        fun = fun.__wrapped__  # Unwrap if decorated\n        fun(instance, *args, **kwargs) if instance else fun(*args, **kwargs)\n    else:\n        fun(*args, **kwargs)\n</code></pre>"},{"location":"api/callback/","title":"Callbacks","text":""},{"location":"api/callback/#callbacks","title":"Callbacks","text":""},{"location":"api/callback/#perceptrain.callbacks.callback.Callback","title":"<code>Callback(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>Base class for defining various training callbacks.</p> ATTRIBUTE DESCRIPTION <code>on</code> <p>The event on which to trigger the callback. Must be a valid on value from: [\"train_start\", \"train_end\",     \"train_epoch_start\", \"train_epoch_end\", \"train_batch_start\",     \"train_batch_end\",\"val_epoch_start\", \"val_epoch_end\",     \"val_batch_start\", \"val_batch_end\", \"test_batch_start\",     \"test_batch_end\"]</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>callback</code> <p>The function to call if the condition is met.</p> <p> TYPE: <code>CallbackFunction | None</code> </p> <code>callback_condition</code> <p>Condition to check before calling.</p> <p> TYPE: <code>CallbackConditionFunction | None</code> </p> <code>modify_optimize_result</code> <p>Function to modify <code>OptimizeResult</code>.</p> <p> TYPE: <code>CallbackFunction | dict[str, Any] | None</code> </p> <p>A callback can be defined in two ways:</p> <ol> <li>By providing a callback function directly in the base class:    This is useful for simple callbacks that don't require subclassing.</li> </ol> <p>Example:    <pre><code>from perceptrain.callbacks import Callback\n\ndef custom_callback_function(trainer, config, writer):\n    print(\"Custom callback executed.\")\n\ncustom_callback = Callback(\n    on=\"train_end\",\n    called_every=5,\n    callback=custom_callback_function\n)\n</code></pre> <pre><code>\n</code></pre> </p> <ol> <li>By inheriting and implementing the <code>run_callback</code> method:    This is suitable for more complex callbacks that require customization.</li> </ol> <p>Example:    <pre><code>from perceptrain.callbacks import Callback\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in the inherited run_callback method.\")\n\ncustom_callback = CustomCallback(on=\"train_end\", called_every=10)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.Callback.on","title":"<code>on</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the TrainingStage.</p> RETURNS DESCRIPTION <code>TrainingStage</code> <p>TrainingStage for the callback</p> <p> TYPE: <code>TrainingStage | str</code> </p>"},{"location":"api/callback/#perceptrain.callbacks.callback.Callback.__call__","title":"<code>__call__(when, trainer, config, writer)</code>","text":"<p>Executes the callback if conditions are met.</p> PARAMETER DESCRIPTION <code>when</code> <p>The event when the callback is triggered.</p> <p> TYPE: <code>str</code> </p> <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback function if executed.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __call__(\n    self, when: TrainingStage, trainer: Any, config: TrainConfig, writer: BaseWriter\n) -&gt; Any:\n    \"\"\"Executes the callback if conditions are met.\n\n    Args:\n        when (str): The event when the callback is triggered.\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback function if executed.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if self.on == when:\n        if opt_result:\n            opt_result = self.modify_optimize_result(opt_result)\n        if self._should_call(when, opt_result):\n            return self.run_callback(trainer, config, writer)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.Callback.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Executes the defined callback.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback execution.</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If not implemented in subclasses.</p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Executes the defined callback.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback execution.\n\n    Raises:\n        NotImplementedError: If not implemented in subclasses.\n    \"\"\"\n    if self.callback is not None:\n        return self.callback(trainer, config, writer)\n    raise NotImplementedError(\"Subclasses should override the run_callback method.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.EarlyStopping","title":"<code>EarlyStopping(on, called_every, monitor, patience=5, mode='min')</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <p>This callback monitors a specified metric (e.g., validation loss or accuracy). If the metric does not improve for a given patience period, training is stopped.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>EarlyStopping</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import EarlyStopping\n\n# Create an instance of the EarlyStopping callback\nearly_stopping = EarlyStopping(on=\"val_epoch_end\",\n                               called_every=1,\n                               monitor=\"val_loss\",\n                               patience=5,\n                               mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[early_stopping]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the EarlyStopping callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"val_epoch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>monitor</code> <p>The metric to monitor (e.g., \"val_loss\" or \"train_loss\"). All metrics returned by optimize step are available to monitor. Please add \"val_\" and \"train_\" strings at the start of the metric name.</p> <p> TYPE: <code>str</code> </p> <code>patience</code> <p>Number of iterations to wait for improvement. Default is 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>mode</code> <p>Whether to minimize (\"min\") or maximize (\"max\") the metric. Default is \"min\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'min'</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self, on: str, called_every: int, monitor: str, patience: int = 5, mode: str = \"min\"\n):\n    \"\"\"Initializes the EarlyStopping callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"val_epoch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n        monitor (str): The metric to monitor (e.g., \"val_loss\" or \"train_loss\").\n            All metrics returned by optimize step are available to monitor.\n            Please add \"val_\" and \"train_\" strings at the start of the metric name.\n        patience (int, optional): Number of iterations to wait for improvement. Default is 5.\n        mode (str, optional): Whether to minimize (\"min\") or maximize (\"max\") the metric.\n            Default is \"min\".\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.monitor = monitor\n    self.patience = patience\n    self.mode = mode\n    self.best_value = float(\"inf\") if mode == \"min\" else -float(\"inf\")\n    self.counter = 0\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.EarlyStopping.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Monitors the metric and stops training if no improvement is observed.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Monitors the metric and stops training if no improvement is observed.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    current_value = trainer.opt_result.metrics.get(self.monitor)\n    if current_value is None:\n        raise ValueError(f\"Metric '{self.monitor}' is not available in the trainer's metrics.\")\n\n    if (self.mode == \"min\" and current_value &lt; self.best_value) or (\n        self.mode == \"max\" and current_value &gt; self.best_value\n    ):\n        self.best_value = current_value\n        self.counter = 0\n    else:\n        self.counter += 1\n\n    if self.counter &gt;= self.patience:\n        logger.info(\n            f\"EarlyStopping: No improvement in '{self.monitor}' for {self.patience} epochs. \"\n            \"Stopping training.\"\n        )\n        trainer._stop_training.fill_(1)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.GradientMonitoring","title":"<code>GradientMonitoring(on, called_every=1)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <p>This callback monitors and logs statistics about the gradients of the model parameters to help debug or optimize the training process.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>GradientMonitoring</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import GradientMonitoring\n\n# Create an instance of the GradientMonitoring callback\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the GradientMonitoring callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"train_batch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int = 1):\n    \"\"\"Initializes the GradientMonitoring callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"train_batch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.GradientMonitoring.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs gradient statistics.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Logs gradient statistics.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        gradient_stats = {}\n        for name, param in trainer.model.named_parameters():\n            if param.grad is not None:\n                grad = param.grad\n                gradient_stats.update(\n                    {\n                        name + \"_mean\": grad.mean().item(),\n                        name + \"_std\": grad.std().item(),\n                        name + \"_max\": grad.max().item(),\n                        name + \"_min\": grad.min().item(),\n                    }\n                )\n\n        writer.write(trainer.opt_result.iteration, gradient_stats)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LRSchedulerCosineAnnealing","title":"<code>LRSchedulerCosineAnnealing(on, called_every, t_max, min_lr=0.0)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies cosine annealing to the learning rate during training.</p> <p>This callback decreases the learning rate following a cosine curve, starting from the initial learning rate and annealing to a minimum (min_lr).</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCosineAnnealing</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LRSchedulerCosineAnnealing\n\n# Create an instance of the LRSchedulerCosineAnnealing callback\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\",\n                                       called_every=1,\n                                       t_max=5000,\n                                       min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cosine]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCosineAnnealing callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>t_max</code> <p>The total number of iterations for one annealing cycle.</p> <p> TYPE: <code>int</code> </p> <code>min_lr</code> <p>The minimum learning rate. Default is 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, t_max: int, min_lr: float = 0.0):\n    \"\"\"Initializes the LRSchedulerCosineAnnealing callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        t_max (int): The total number of iterations for one annealing cycle.\n        min_lr (float, optional): The minimum learning rate. Default is 0.0.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.t_max = t_max\n    self.min_lr = min_lr\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LRSchedulerCosineAnnealing.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate using cosine annealing.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate using cosine annealing.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        max_lr = param_group[\"lr\"]\n        new_lr = (\n            self.min_lr\n            + (max_lr - self.min_lr)\n            * (1 + math.cos(math.pi * trainer.opt_result.iteration / self.t_max))\n            / 2\n        )\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LRSchedulerCyclic","title":"<code>LRSchedulerCyclic(on, called_every, base_lr, max_lr, step_size)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies a cyclic learning rate schedule during training.</p> <p>This callback oscillates the learning rate between a minimum (base_lr) and a maximum (max_lr) over a defined cycle length (step_size). The learning rate follows a triangular wave pattern.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCyclic</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LRSchedulerCyclic\n\n# Create an instance of the LRSchedulerCyclic callback\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\",\n                              called_every=1,\n                              base_lr=0.001,\n                              max_lr=0.01,\n                              step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cyclic]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCyclic callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>base_lr</code> <p>The minimum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>max_lr</code> <p>The maximum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>step_size</code> <p>Number of iterations for half a cycle.</p> <p> TYPE: <code>int</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, base_lr: float, max_lr: float, step_size: int):\n    \"\"\"Initializes the LRSchedulerCyclic callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        base_lr (float): The minimum learning rate.\n        max_lr (float): The maximum learning rate.\n        step_size (int): Number of iterations for half a cycle.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.base_lr = base_lr\n    self.max_lr = max_lr\n    self.step_size = step_size\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LRSchedulerCyclic.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate cyclically.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate cyclically.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    cycle = trainer.opt_result.iteration // (2 * self.step_size)\n    x = abs(trainer.opt_result.iteration / self.step_size - 2 * cycle - 1)\n    scale = max(0, (1 - x))\n    new_lr = self.base_lr + (self.max_lr - self.base_lr) * scale\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LRSchedulerStepDecay","title":"<code>LRSchedulerStepDecay(on, called_every, gamma=0.5)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Reduces the learning rate by a factor at regular intervals.</p> <p>This callback adjusts the learning rate by multiplying it with a decay factor after a specified number of iterations. The learning rate is updated as:     lr = lr * gamma</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerStepDecay</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LRSchedulerStepDecay\n\n# Create an instance of the LRSchedulerStepDecay callback\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\",\n                                     called_every=100,\n                                     gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_step_decay]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerStepDecay callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>gamma</code> <p>The decay factor applied to the learning rate. A value &lt; 1 reduces the learning rate over time. Default is 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, gamma: float = 0.5):\n    \"\"\"Initializes the LRSchedulerStepDecay callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        gamma (float, optional): The decay factor applied to the learning rate.\n            A value &lt; 1 reduces the learning rate over time. Default is 0.5.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.gamma = gamma\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LRSchedulerStepDecay.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Runs the callback to apply step decay to the learning rate.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Runs the callback to apply step decay to the learning rate.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] *= self.gamma\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LoadCheckpoint","title":"<code>LoadCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to load a model checkpoint.</p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LoadCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Loads a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The result of loading the checkpoint.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Loads a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: The result of loading the checkpoint.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        folder = config.log_folder\n        model = trainer.model\n        optimizer = trainer.optimizer\n        device = trainer.accelerator.execution.log_device\n        return load_checkpoint(folder, model, optimizer, device=device)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LogHyperparameters","title":"<code>LogHyperparameters(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log hyperparameters using the writer.</p> <p>The <code>LogHyperparameters</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LogHyperparameters</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LogHyperparameters\n\n# Create an instance of the LogHyperparameters callback\nlog_hyper_callback = LogHyperparameters(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[log_hyper_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LogHyperparameters.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs hyperparameters using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs hyperparameters using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        hyperparams = config.hyperparams\n        writer.log_hyperparams(hyperparams)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LogModelTracker","title":"<code>LogModelTracker(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log the model using the writer.</p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.LogModelTracker.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs the model using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs the model using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        model = trainer.model\n        writer.log_model(\n            model, trainer.train_dataloader, trainer.val_dataloader, trainer.test_dataloader\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.PlotMetrics","title":"<code>PlotMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to plot metrics using the writer.</p> <p>The <code>PlotMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PlotMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import PlotMetrics\n\n# Create an instance of the PlotMetrics callback\nplot_metrics_callback = PlotMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[plot_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.PlotMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Plots metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Plots metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        opt_result = trainer.opt_result\n        plotting_functions = config.plotting_functions\n        writer.plot(trainer.model, opt_result.iteration, plotting_functions)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.PrintMetrics","title":"<code>PrintMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to print metrics using the writer.</p> <p>The <code>PrintMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PrintMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import PrintMetrics\n\n# Create an instance of the PrintMetrics callback\nprint_metrics_callback = PrintMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[print_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.PrintMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Prints metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Prints metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.print_metrics(opt_result)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.SaveBestCheckpoint","title":"<code>SaveBestCheckpoint(on, called_every)</code>","text":"<p>               Bases: <code>SaveCheckpoint</code></p> <p>Callback to save the best model checkpoint based on a validation criterion.</p> <p>Initializes the SaveBestCheckpoint callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int):\n    \"\"\"Initializes the SaveBestCheckpoint callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.best_loss = float(\"inf\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.SaveBestCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves the checkpoint if the current loss is better than the best loss.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves the checkpoint if the current loss is better than the best loss.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        opt_result = trainer.opt_result\n        if config.validation_criterion and config.validation_criterion(\n            opt_result.loss, self.best_loss, config.val_epsilon\n        ):\n            self.best_loss = opt_result.loss\n\n            folder = config.log_folder\n            model = trainer.model\n            optimizer = trainer.optimizer\n            opt_result = trainer.opt_result\n            write_checkpoint(folder, model, optimizer, \"best\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.SaveCheckpoint","title":"<code>SaveCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to save a model checkpoint.</p> <p>The <code>SaveCheckpoint</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>SaveCheckpoint</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import SaveCheckpoint\n\n# Create an instance of the SaveCheckpoint callback\nsave_checkpoint_callback = SaveCheckpoint(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.SaveCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        folder = config.log_folder\n        model = trainer.model\n        optimizer = trainer.optimizer\n        opt_result = trainer.opt_result\n        write_checkpoint(folder, model, optimizer, opt_result.iteration)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.WriteMetrics","title":"<code>WriteMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to write metrics using the writer.</p> <p>The <code>WriteMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>WriteMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import WriteMetrics\n\n# Create an instance of the WriteMetrics callback\nwrite_metrics_callback = WriteMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[write_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = (\n        callback_condition if callback_condition else Callback.default_callback\n    )\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = lambda opt_res: Callback.modify_opt_res_dict(\n            opt_res, modify_optimize_result\n        )\n    else:\n        self.modify_optimize_result = (\n            modify_optimize_result\n            if modify_optimize_result\n            else Callback.modify_opt_res_default\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.callback.WriteMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Writes metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Writes metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    if trainer.accelerator.rank == 0:\n        opt_result = trainer.opt_result\n        writer.write(opt_result.iteration, opt_result.metrics)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for experiment tracking writers.</p> METHOD DESCRIPTION <code>open</code> <p>Opens the writer and sets up the logging environment.</p> <code>close</code> <p>Closes the writer and finalizes any ongoing logging processes.</p> <code>print_metrics</code> <p>Prints metrics and loss in a formatted manner.</p> <code>write</code> <p>Writes the optimization results to the tracking tool.</p> <code>log_hyperparams</code> <p>Logs the hyperparameters to the tracking tool.</p> <code>plot</code> <p>Logs model plots using provided plotting functions.</p> <code>log_model</code> <p>Logs the model and any relevant information.</p>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Closes the writer and finalizes logging.</p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Closes the writer and finalizes logging.\"\"\"\n    raise NotImplementedError(\"Writers must implement a close method.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Logs hyperparameters.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_hyperparams method.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>  <code>abstractmethod</code>","text":"<p>Logs the model and associated data.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and associated data.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_model method.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter.open","title":"<code>open(config, iteration=None)</code>  <code>abstractmethod</code>","text":"<p>Opens the writer and prepares it for logging.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef open(self, config: TrainConfig, iteration: int | None = None) -&gt; Any:\n    \"\"\"\n    Opens the writer and prepares it for logging.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement an open method.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>  <code>abstractmethod</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used to\n            generate plots.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a plot method.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter.print_metrics","title":"<code>print_metrics(result)</code>","text":"<p>Prints the metrics and loss in a readable format.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to display.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def print_metrics(self, result: OptimizeResult) -&gt; None:\n    \"\"\"Prints the metrics and loss in a readable format.\n\n    Args:\n        result (OptimizeResult): The optimization results to display.\n    \"\"\"\n\n    # Find the key in result.metrics that contains \"loss\" (case-insensitive)\n    loss_key = next((k for k in result.metrics if \"loss\" in k.lower()), None)\n    initial = f\"P {result.rank: &gt;2}|{result.device: &lt;7}| Iteration {result.iteration: &gt;7}| \"\n    if loss_key:\n        loss_value = result.metrics[loss_key]\n        msg = initial + f\"{loss_key.title()}: {loss_value:.7f} -\"\n    else:\n        msg = initial + f\"Loss: None -\"\n    msg += \" \".join([f\"{k}: {v:.7f}\" for k, v in result.metrics.items() if k != loss_key])\n    print(msg)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.BaseWriter.write","title":"<code>write(iteration, metrics)</code>  <code>abstractmethod</code>","text":"<p>Logs the results of the current iteration.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a write method.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter","title":"<code>MLFlowWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to MLflow.</p> ATTRIBUTE DESCRIPTION <code>run</code> <p>The active MLflow run.</p> <p> TYPE: <code>Run</code> </p> <code>mlflow</code> <p>The MLflow module.</p> <p> TYPE: <code>ModuleType</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    try:\n        from mlflow.entities import Run\n    except ImportError:\n        raise ImportError(\n            \"mlflow is not installed. Please install perceptrain with the mlflow feature: \"\n            \"`pip install perceptrain[mlflow]`.\"\n        )\n\n    self.run: Run\n    self.mlflow: ModuleType\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter.close","title":"<code>close()</code>","text":"<p>Closes the MLflow run.</p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the MLflow run.\"\"\"\n    if self.run:\n        self.mlflow.end_run()\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter.get_signature_from_dataloader","title":"<code>get_signature_from_dataloader(model, dataloader)</code>","text":"<p>Infers the signature of the model based on the input data from the dataloader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to use for inference.</p> <p> TYPE: <code>Module</code> </p> <code>dataloader</code> <p>DataLoader for model inputs.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Optional[Any]: The inferred signature, if available.</p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def get_signature_from_dataloader(\n    self, model: Module, dataloader: DataLoader | DictDataLoader | None\n) -&gt; Any:\n    \"\"\"\n    Infers the signature of the model based on the input data from the dataloader.\n\n    Args:\n        model (Module): The model to use for inference.\n        dataloader (DataLoader | DictDataLoader |  None): DataLoader for model inputs.\n\n    Returns:\n        Optional[Any]: The inferred signature, if available.\n    \"\"\"\n    from mlflow.models import infer_signature\n\n    if dataloader is None:\n        return None\n\n    xs: InputData\n    xs, *_ = next(iter(dataloader))\n    preds = model(xs)\n\n    if isinstance(xs, Tensor):\n        xs = xs.detach().cpu().numpy()\n        preds = preds.detach().cpu().numpy()\n        return infer_signature(xs, preds)\n\n    return None\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to MLflow.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to MLflow.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_params(hyperparams)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model and its signature to MLflow using the provided data loaders.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and its signature to MLflow using the provided data loaders.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    if not self.mlflow:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n\n    signatures = self.get_signature_from_dataloader(model, train_dataloader)\n    self.mlflow.pytorch.log_model(model, artifact_path=\"model\", signature=signatures)\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the MLflow writer and initializes an MLflow run.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>mlflow</code> <p>The MLflow module instance.</p> <p> TYPE: <code>ModuleType | None</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; ModuleType | None:\n    \"\"\"\n    Opens the MLflow writer and initializes an MLflow run.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        mlflow: The MLflow module instance.\n    \"\"\"\n    import mlflow\n\n    self.mlflow = mlflow\n    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"\")\n    experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", str(uuid4()))\n    run_name = os.getenv(\"MLFLOW_RUN_NAME\", str(uuid4()))\n\n    if self.mlflow:\n        self.mlflow.set_tracking_uri(tracking_uri)\n\n        # Create or get the experiment\n        exp_filter_string = f\"name = '{experiment_name}'\"\n        experiments = self.mlflow.search_experiments(filter_string=exp_filter_string)\n        if not experiments:\n            self.mlflow.create_experiment(name=experiment_name)\n\n        self.mlflow.set_experiment(experiment_name)\n        self.run = self.mlflow.start_run(run_name=run_name, nested=False)\n\n    return self.mlflow\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.mlflow:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.mlflow.log_figure(fig, descr)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.MLFlowWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to MLflow.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to MLflow.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_metrics(metrics, step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.TensorBoardWriter","title":"<code>TensorBoardWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to TensorBoard.</p> ATTRIBUTE DESCRIPTION <code>writer</code> <p>The TensorBoard SummaryWriter instance.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.writer = None\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.TensorBoardWriter.close","title":"<code>close()</code>","text":"<p>Closes the TensorBoard writer.</p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the TensorBoard writer.\"\"\"\n    if self.writer:\n        self.writer.close()\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.TensorBoardWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to TensorBoard.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to TensorBoard.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.writer:\n        self.writer.add_hparams(hyperparams, {})\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.TensorBoardWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model.</p> <p>Currently not supported by TensorBoard.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model.\n\n    Currently not supported by TensorBoard.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    logger.warning(\"Model logging is not supported by tensorboard. No model will be logged.\")\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.TensorBoardWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the TensorBoard writer.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SummaryWriter</code> <p>The initialized TensorBoard writer.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; SummaryWriter:\n    \"\"\"\n    Opens the TensorBoard writer.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        SummaryWriter: The initialized TensorBoard writer.\n    \"\"\"\n    log_dir = str(config.log_folder)\n    purge_step = iteration if isinstance(iteration, int) else None\n    self.writer = SummaryWriter(log_dir=log_dir, purge_step=purge_step)\n    return self.writer\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.TensorBoardWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.writer:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.writer.add_figure(descr, fig, global_step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.TensorBoardWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to TensorBoard.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to TensorBoard.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.writer:\n        for key, value in metrics.items():\n            self.writer.add_scalar(key, value, iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/callback/#perceptrain.callbacks.writer_registry.get_writer","title":"<code>get_writer(tracking_tool)</code>","text":"<p>Factory method to get the appropriate writer based on the tracking tool.</p> PARAMETER DESCRIPTION <code>tracking_tool</code> <p>The experiment tracking tool to use.</p> <p> TYPE: <code>ExperimentTrackingTool</code> </p> RETURNS DESCRIPTION <code>BaseWriter</code> <p>An instance of the appropriate writer.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>perceptrain/callbacks/writer_registry.py</code> <pre><code>def get_writer(tracking_tool: ExperimentTrackingTool) -&gt; BaseWriter:\n    \"\"\"Factory method to get the appropriate writer based on the tracking tool.\n\n    Args:\n        tracking_tool (ExperimentTrackingTool): The experiment tracking tool to use.\n\n    Returns:\n        BaseWriter: An instance of the appropriate writer.\n    \"\"\"\n    writer_class = WRITER_REGISTRY.get(tracking_tool)\n    if writer_class:\n        return writer_class()\n    else:\n        raise ValueError(f\"Unsupported tracking tool: {tracking_tool}\")\n</code></pre>"},{"location":"api/config/","title":"Configurations","text":""},{"location":"api/config/#data-and-configurations","title":"Data and configurations","text":"<p>Default Torch optimize step with closure.</p> <p>This is the default optimization step.</p> PARAMETER DESCRIPTION <code>model</code> <p>The input model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The chosen Torch optimizer.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable</code> </p> <code>xs</code> <p>The input data. If None, it means the given model does not require any input data.</p> <p> TYPE: <code>dict | list | Tensor | None</code> </p> <code>device</code> <p>A target device to run computations on.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for <code>xs</code> conversion.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor | float, dict | None]</code> <p>tuple[Tensor | float, dict | None]: A tuple containing the computed loss value and a dictionary with collected metrics.</p> Source code in <code>perceptrain/optimize_step.py</code> <pre><code>def optimize_step(\n    model: Module,\n    optimizer: Optimizer,\n    loss_fn: Callable,\n    xs: dict | list | torch.Tensor | None,\n    device: torch.device = None,\n    dtype: torch.dtype = None,\n) -&gt; tuple[torch.Tensor | float, dict | None]:\n    \"\"\"Default Torch optimize step with closure.\n\n    This is the default optimization step.\n\n    Args:\n        model (Module): The input model to be optimized.\n        optimizer (Optimizer): The chosen Torch optimizer.\n        loss_fn (Callable): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        xs (dict | list | Tensor | None): The input data. If None, it means\n            the given model does not require any input data.\n        device (torch.device): A target device to run computations on.\n        dtype (torch.dtype): Data type for `xs` conversion.\n\n    Returns:\n        tuple[Tensor | float, dict | None]: A tuple containing the computed loss value\n            and a dictionary with collected metrics.\n    \"\"\"\n\n    loss, metrics = None, {}\n\n    def closure() -&gt; Any:\n        # NOTE: We need the nonlocal as we can't return a metric dict and\n        # because e.g. LBFGS calls this closure multiple times but for some\n        # reason the returned loss is always the first one...\n        nonlocal metrics, loss\n        optimizer.zero_grad()\n        loss, metrics = loss_fn(model, xs)\n        loss.backward(retain_graph=True)\n        return loss.item()\n\n    optimizer.step(closure)\n    # return the loss/metrics that are being mutated inside the closure...\n    return loss, metrics\n</code></pre>"},{"location":"api/config/#perceptrain.config.TrainConfig","title":"<code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=lambda: list()(), log_model=False, root_folder=Path('./qml_logs'), create_subfolder_per_run=False, log_folder=Path('./'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=ExperimentTrackingTool.TENSORBOARD, hyperparams=dict(), plotting_functions=tuple(), _subfolders=list(), nprocs=1, compute_setup='cpu', backend='gloo', log_setup='cpu', dtype=None, all_reduce_metrics=False)</code>  <code>dataclass</code>","text":"<p>Default configuration for the training process.</p> <p>This class provides default settings for various aspects of the training loop, such as logging, checkpointing, and validation. The default values for these fields can be customized when an instance of <code>TrainConfig</code> is created.</p> <p>Example: <pre><code>from perceptrain import TrainConfig\nc = TrainConfig(root_folder=\"/tmp/train\")\n</code></pre> <pre><code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=[], log_model=False, root_folder='/tmp/train', create_subfolder_per_run=False, log_folder=PosixPath('.'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=&lt;ExperimentTrackingTool.TENSORBOARD: 'tensorboard'&gt;, hyperparams={}, plotting_functions=(), _subfolders=[], nprocs=1, compute_setup='cpu', backend='gloo', log_setup='cpu', dtype=None, all_reduce_metrics=False)\n</code></pre> </p>"},{"location":"api/config/#perceptrain.config.TrainConfig.all_reduce_metrics","title":"<code>all_reduce_metrics = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to aggregate metrics (e.g., loss, accuracy) across processes.</p> <p>When True, metrics from different training processes are averaged to provide a consolidated metrics. Note: Since aggregation requires synchronization/all_reduce operation, this can increase the  computation time significantly.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.backend","title":"<code>backend = 'gloo'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backend used for distributed training communication.</p> <p>The default is \"gloo\". Other options may include \"nccl\" - which is optimized for GPU-based training or \"mpi\", depending on your system and requirements. It should be one of the backends supported by <code>torch.distributed</code>. For further details, please look at torch backends</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.batch_size","title":"<code>batch_size = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The batch size to use when processing a list or tuple of torch.Tensors.</p> <p>This specifies how many samples are processed in each training iteration.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.callbacks","title":"<code>callbacks = field(default_factory=lambda: list())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of callbacks to execute during training.</p> <p>Callbacks can be used for custom behaviors, such as early stopping, custom logging, or other actions triggered at specific events.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.checkpoint_best_only","title":"<code>checkpoint_best_only = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, checkpoints are only saved if there is an improvement in the.</p> <p>validation metric. This conserves storage by only keeping the best models.</p> <p>validation_criterion is required when this is set to True.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.checkpoint_every","title":"<code>checkpoint_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for saving model and optimizer checkpoints during training.</p> <p>Set to 0 to disable checkpointing. This helps in resuming training or recovering models. Note that setting checkpoint_best_only = True will disable this and only best checkpoints will be saved.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.compute_setup","title":"<code>compute_setup = 'cpu'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Compute device setup; options are \"auto\", \"gpu\", or \"cpu\".</p> <ul> <li>\"auto\": Automatically uses GPU if available; otherwise, falls back to CPU.</li> <li>\"gpu\": Forces GPU usage, raising an error if no CUDA device is available.</li> <li>\"cpu\": Forces the use of CPU regardless of GPU availability.</li> </ul>"},{"location":"api/config/#perceptrain.config.TrainConfig.create_subfolder_per_run","title":"<code>create_subfolder_per_run = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to create a subfolder for each run, named <code>&lt;id&gt;_&lt;timestamp&gt;_&lt;PID&gt;</code>.</p> <p>This ensures logs and checkpoints from different runs do not overwrite each other, which is helpful for rapid prototyping. If <code>False</code>, training will resume from the latest checkpoint if one exists in the specified log folder.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.dtype","title":"<code>dtype = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Data type (precision) for computations.</p> <p>Both model parameters, and dataset will be of the provided precision.</p> <p>If not specified or None, the default torch precision (usually torch.float32) is used. If provided dtype is torch.complex128, model parameters will be torch.complex128, and data parameters will be torch.float64</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.hyperparams","title":"<code>hyperparams = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of hyperparameters to be tracked.</p> <p>This can include learning rates, regularization parameters, or any other training-related configurations.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.log_folder","title":"<code>log_folder = Path('./')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The log folder for saving checkpoints and tensorboard logs.</p> <p>This stores the path where all logs and checkpoints are being saved for this training session. <code>log_folder</code> takes precedence over <code>root_folder</code>, but it is ignored if <code>create_subfolders_per_run=True</code> (in which case, subfolders will be spawned in the root folder).</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.log_model","title":"<code>log_model = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to log a serialized version of the model.</p> <p>When set to <code>True</code>, the model's state will be logged, useful for model versioning and reproducibility.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.log_setup","title":"<code>log_setup = 'cpu'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Logging device setup; options are \"auto\" or \"cpu\".</p> <ul> <li>\"auto\": Uses the same device for logging as for computation.</li> <li>\"cpu\": Forces logging to occur on the CPU. This can be useful to avoid potential conflicts with GPU processes.</li> </ul>"},{"location":"api/config/#perceptrain.config.TrainConfig.max_iter","title":"<code>max_iter = 10000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of training iterations (epochs) to perform.</p> <p>This defines the total number of times the model will be updated.</p> <p>In case of InfiniteTensorDataset, each epoch will have 1 batch. In case of TensorDataset, each epoch will have len(dataloader) batches.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.nprocs","title":"<code>nprocs = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The number of processes to use for training when spawning subprocesses.</p> <p>For effective parallel processing, set this to a value greater than 1. - In case of Multi-GPU or Multi-Node-Multi-GPU setups, nprocs should be equal to the total number of GPUs across all nodes (world size), or total number of GPU to be used.</p> <p>If nprocs &gt; 1, multiple processes will be spawned for training. The training framework will launch additional processes (e.g., for distributed or parallel training). - For CPU setup, this will launch a true parallel processes - For GPU setup, this will launch a distributed training routine. This uses the DistributedDataParallel framework from PyTorch.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.plot_every","title":"<code>plot_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for generating and saving figures during training.</p> <p>Set to 0 to disable plotting.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.plotting_functions","title":"<code>plotting_functions = field(default_factory=tuple)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Functions used for in-training plotting.</p> <p>These are called to generate plots that are logged or saved at specified intervals.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.print_every","title":"<code>print_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for printing loss and metrics to the console during training.</p> <p>Set to 0 to disable this output, meaning that metrics and loss will not be printed during training.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.root_folder","title":"<code>root_folder = Path('./qml_logs')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The root folder for saving checkpoints and tensorboard logs.</p> <p>The default path is \"./qml_logs\"</p> <p>This can be set to a specific directory where training artifacts are to be stored. Checkpoints will be saved inside a subfolder in this directory. Subfolders will be created based on <code>create_subfolder_per_run</code> argument.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.tracking_tool","title":"<code>tracking_tool = ExperimentTrackingTool.TENSORBOARD</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The tool used for tracking training progress and logging metrics.</p> <p>Options include tools like TensorBoard, which help visualize and monitor model training.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.trainstop_criterion","title":"<code>trainstop_criterion = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to determine if the training process should stop based on a.</p> <p>specific stopping metric. If <code>None</code>, training continues until <code>max_iter</code> is reached.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.val_epsilon","title":"<code>val_epsilon = 1e-05</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A small safety margin used to compare the current validation loss with the.</p> <p>best previous validation loss. This is used to determine improvements in metrics.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.val_every","title":"<code>val_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for performing validation.</p> <p>If set to 0, validation is not performed. Note that metrics from validation are always written, regardless of the <code>write_every</code> setting. Note that initial validation happens at the start of training (when val_every &gt; 0)     For initial validation  - initial metrics are written.                             - checkpoint is saved (when checkpoint_best_only = False)</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.validation_criterion","title":"<code>validation_criterion = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to evaluate whether a given validation metric meets a desired condition.</p> <p>The validation_criterion has the following format: def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:     # process</p> <p>If <code>None</code>, no custom validation criterion is applied.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.verbose","title":"<code>verbose = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to print metrics and status messages during training.</p> <p>If <code>True</code>, detailed metrics and status updates will be displayed in the console.</p>"},{"location":"api/config/#perceptrain.config.TrainConfig.write_every","title":"<code>write_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for writing loss and metrics using the tracking tool during training.</p> <p>Set to 0 to disable this logging, which prevents metrics from being logged to the tracking tool. Note that the metrics will always be written at the end of training regardless of this setting.</p>"},{"location":"api/config/#perceptrain.parameters.get_parameters","title":"<code>get_parameters(model)</code>","text":"<p>Retrieve all trainable model parameters in a single vector.</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a 1-dimensional tensor with the parameters</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>perceptrain/parameters.py</code> <pre><code>def get_parameters(model: Module) -&gt; Tensor:\n    \"\"\"Retrieve all trainable model parameters in a single vector.\n\n    Args:\n        model (Module): the input PyTorch model\n\n    Returns:\n        Tensor: a 1-dimensional tensor with the parameters\n    \"\"\"\n    ps = [p.reshape(-1) for p in model.parameters() if p.requires_grad]\n    return torch.concat(ps)\n</code></pre>"},{"location":"api/config/#perceptrain.parameters.num_parameters","title":"<code>num_parameters(model)</code>","text":"<p>Return the total number of parameters of the given model.</p> Source code in <code>perceptrain/parameters.py</code> <pre><code>def num_parameters(model: Module) -&gt; int:\n    \"\"\"Return the total number of parameters of the given model.\"\"\"\n    return len(get_parameters(model))\n</code></pre>"},{"location":"api/config/#perceptrain.parameters.set_parameters","title":"<code>set_parameters(model, theta)</code>","text":"<p>Set all trainable parameters of a model from a single vector.</p> <p>Notice that this function assumes prior knowledge of right number of parameters in the model</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> <code>theta</code> <p>the parameters to assign</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>perceptrain/parameters.py</code> <pre><code>def set_parameters(model: Module, theta: Tensor) -&gt; None:\n    \"\"\"Set all trainable parameters of a model from a single vector.\n\n    Notice that this function assumes prior knowledge of right number\n    of parameters in the model\n\n    Args:\n        model (Module): the input PyTorch model\n        theta (Tensor): the parameters to assign\n    \"\"\"\n\n    with torch.no_grad():\n        idx = 0\n        for ps in model.parameters():\n            if ps.requires_grad:\n                n = torch.numel(ps)\n                if ps.ndim == 0:\n                    ps[()] = theta[idx : idx + n]\n                else:\n                    ps[:] = theta[idx : idx + n].reshape(ps.size())\n                idx += n\n</code></pre>"},{"location":"api/config/#perceptrain.data.DictDataLoader","title":"<code>DictDataLoader(dataloaders)</code>  <code>dataclass</code>","text":"<p>This class only holds a dictionary of <code>DataLoader</code>s and samples from them.</p>"},{"location":"api/config/#perceptrain.data.InfiniteTensorDataset","title":"<code>InfiniteTensorDataset(*tensors)</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Randomly sample points from the first dimension of the given tensors.</p> <p>Behaves like a normal torch <code>Dataset</code> just that we can sample from it as many times as we want.</p> <p>Examples: <pre><code>import torch\nfrom perceptrain.data import InfiniteTensorDataset\n\nx_data, y_data = torch.rand(5,2), torch.ones(5,1)\n# The dataset accepts any number of tensors with the same batch dimension\nds = InfiniteTensorDataset(x_data, y_data)\n\n# call `next` to get one sample from each tensor:\nxs = next(iter(ds))\n</code></pre> <pre><code>(tensor([0.0989, 0.9692]), tensor([1.]))\n</code></pre></p> Source code in <code>perceptrain/data.py</code> <pre><code>def __init__(self, *tensors: Tensor):\n    \"\"\"Randomly sample points from the first dimension of the given tensors.\n\n    Behaves like a normal torch `Dataset` just that we can sample from it as\n    many times as we want.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from perceptrain.data import InfiniteTensorDataset\n\n    x_data, y_data = torch.rand(5,2), torch.ones(5,1)\n    # The dataset accepts any number of tensors with the same batch dimension\n    ds = InfiniteTensorDataset(x_data, y_data)\n\n    # call `next` to get one sample from each tensor:\n    xs = next(iter(ds))\n    print(str(xs)) # markdown-exec: hide\n    ```\n    \"\"\"\n    self.tensors = tensors\n    self.indices = list(range(self.tensors[0].size(0)))\n</code></pre>"},{"location":"api/config/#perceptrain.data.OptimizeResult","title":"<code>OptimizeResult(iteration, model, optimizer, loss=None, metrics=lambda: dict()(), extra=lambda: dict()(), rank=0, device='cpu')</code>  <code>dataclass</code>","text":"<p>OptimizeResult stores many optimization intermediate values.</p> <p>We store at a current iteration, the model, optimizer, loss values, metrics. An extra dict can be used for saving other information to be used for callbacks.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.device","title":"<code>device = 'cpu'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Device on which this result for calculated.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.extra","title":"<code>extra = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Extra dict for saving anything else to be used in callbacks.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.iteration","title":"<code>iteration</code>  <code>instance-attribute</code>","text":"<p>Current iteration number.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.loss","title":"<code>loss = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Loss value.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.metrics","title":"<code>metrics = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that can be saved during training.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model at iteration.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.optimizer","title":"<code>optimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer at iteration.</p>"},{"location":"api/config/#perceptrain.data.OptimizeResult.rank","title":"<code>rank = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Rank of the process for which this result was generated.</p>"},{"location":"api/config/#perceptrain.data.data_to_device","title":"<code>data_to_device(xs, *args, **kwargs)</code>","text":"<p>Utility method to move arbitrary data to 'device'.</p> Source code in <code>perceptrain/data.py</code> <pre><code>@singledispatch\ndef data_to_device(xs: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Utility method to move arbitrary data to 'device'.\"\"\"\n    raise ValueError(f\"Unable to move {type(xs)} with input args: {args} and kwargs: {kwargs}.\")\n</code></pre>"},{"location":"api/config/#perceptrain.data.to_dataloader","title":"<code>to_dataloader(*tensors, batch_size=1, infinite=False)</code>","text":"<p>Convert torch tensors an (infinite) Dataloader.</p> PARAMETER DESCRIPTION <code>*tensors</code> <p>Torch tensors to use in the dataloader.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>()</code> </p> <code>batch_size</code> <p>batch size of sampled tensors</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>infinite</code> <p>if <code>True</code>, the dataloader will keep sampling indefinitely even after the whole dataset was sampled once</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>import torch\nfrom perceptrain import to_dataloader\n\n(x, y, z) = [torch.rand(10) for _ in range(3)]\nloader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\nprint(next(loader))\nprint(next(loader))\nprint(next(loader))\n</code></pre> <pre><code>[tensor([0.1474, 0.2621, 0.1685, 0.5423, 0.6097]), tensor([0.5223, 0.4269, 0.4231, 0.8404, 0.6672]), tensor([0.5368, 0.5488, 0.3991, 0.3324, 0.4773])]\n[tensor([0.3876, 0.9871, 0.2149, 0.5164, 0.4854]), tensor([0.3471, 0.0761, 0.1323, 0.0925, 0.8949]), tensor([0.3008, 0.0808, 0.6298, 0.6233, 0.1224])]\n[tensor([0.1474, 0.2621, 0.1685, 0.5423, 0.6097]), tensor([0.5223, 0.4269, 0.4231, 0.8404, 0.6672]), tensor([0.5368, 0.5488, 0.3991, 0.3324, 0.4773])]\n</code></pre> Source code in <code>perceptrain/data.py</code> <pre><code>def to_dataloader(*tensors: Tensor, batch_size: int = 1, infinite: bool = False) -&gt; DataLoader:\n    \"\"\"Convert torch tensors an (infinite) Dataloader.\n\n    Arguments:\n        *tensors: Torch tensors to use in the dataloader.\n        batch_size: batch size of sampled tensors\n        infinite: if `True`, the dataloader will keep sampling indefinitely even after the whole\n            dataset was sampled once\n\n    Examples:\n\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from perceptrain import to_dataloader\n\n    (x, y, z) = [torch.rand(10) for _ in range(3)]\n    loader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\n    print(next(loader))\n    print(next(loader))\n    print(next(loader))\n    ```\n    \"\"\"\n    ds = InfiniteTensorDataset(*tensors) if infinite else TensorDataset(*tensors)\n    return DataLoader(ds, batch_size=batch_size)\n</code></pre>"},{"location":"api/config/#perceptrain.models.QNN","title":"<code>QNN()</code>","text":"<p>               Bases: <code>QuantumModel</code></p> <p>A specialized quantum neural network that extends QuantumModel.</p> <p>You can define additional layers, parameters, and logic specific to your quantum model here.</p> Source code in <code>perceptrain/models.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n</code></pre>"},{"location":"api/config/#perceptrain.models.QNN.forward","title":"<code>forward(x)</code>","text":"<p>The forward pass for the quantum neural network.</p> <p>Replace with your actual quantum circuit logic if you have a quantum simulator or hardware integration. This example just passes x through a classical linear layer.</p> Source code in <code>perceptrain/models.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    The forward pass for the quantum neural network.\n\n    Replace with your actual quantum circuit logic if you have a\n    quantum simulator or hardware integration. This example just\n    passes x through a classical linear layer.\n    \"\"\"\n    return x\n</code></pre>"},{"location":"api/config/#perceptrain.models.QuantumModel","title":"<code>QuantumModel()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for any quantum-based model.</p> <p>Inherits from nn.Module. Subclasses should implement a forward method that handles quantum logic.</p> Source code in <code>perceptrain/models.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n</code></pre>"},{"location":"api/config/#perceptrain.models.QuantumModel.forward","title":"<code>forward(x)</code>","text":"<p>Override this method in subclasses to provide.</p> <p>the forward pass for your quantum model.</p> Source code in <code>perceptrain/models.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Override this method in subclasses to provide.\n\n    the forward pass for your quantum model.\n    \"\"\"\n    return x\n</code></pre>"},{"location":"api/information/","title":"Information Content","text":""},{"location":"api/information/#information-content","title":"Information Content","text":""},{"location":"api/information/#perceptrain.information.information_content.InformationContent","title":"<code>InformationContent(model, loss_fn, xs, epsilons, variation_multiple=20)</code>","text":"<p>Information Landscape class.</p> <p>This class handles the study of loss landscape from information theoretic perspective and provides methods to get bounds on the norm of the gradient from the Information Content of the loss landscape.</p> PARAMETER DESCRIPTION <code>model</code> <p>The quantum or classical model to analyze.</p> <p> TYPE: <code>Module</code> </p> <code>loss_fn</code> <p>Loss function that takes model output and calculates loss</p> <p> TYPE: <code>Callable</code> </p> <code>xs</code> <p>Input data to evaluate the model on</p> <p> TYPE: <code>Any</code> </p> <code>epsilons</code> <p>The thresholds to use for discretization of the finite derivatives</p> <p> TYPE: <code>Tensor</code> </p> <code>variation_multiple</code> <p>The number of sets of variational parameters to generate per each variational parameter. The number of variational parameters required for the statistical analysis scales linearly with the amount of them present in the model. This is that linear factor.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> Notes <p>This class provides flexibility in terms of what the model, the loss function, and the xs are. The only requirement is that the loss_fn takes the model and xs as arguments and returns the loss, and another dictionary of other metrics.</p> <p>Thus, assumed structure:     loss_fn(model, xs) -&gt; (loss, metrics, ...)</p> <p>Example: A Classifier     <pre><code>model = nn.Linear(10, 1)\n\ndef loss_fn(\n    model: nn.Module,\n    xs: tuple[torch.Tensor, torch.Tensor]\n) -&gt; tuple[torch.Tensor, dict[str, float]:\n    criterion = nn.MSELoss()\n    inputs, labels = xs\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    metrics = {\"loss\": loss.item()}\n    return loss, metrics\n\nxs = (torch.randn(10, 10), torch.randn(10, 1))\n\ninfo_landscape = InfoLandscape(model, loss_fn, xs)\n</code></pre>     In this example, the model is a linear classifier, and the <code>xs</code> include both the     inputs and the target labels. The logic for calculation of the loss from this lies     entirely within the <code>loss_fn</code> function. This can then further be used to obtain the     bounds on the average norm of the gradient of the loss function.</p> <p>Example: A Physics Informed Neural Network     <pre><code>class PhysicsInformedNN(nn.Module):\n    // &lt;Initialization Logic&gt;\n\n    def forward(self, xs: dict[str, torch.Tensor]):\n        return {\n            \"pde_residual\": pde_residual(xs[\"pde\"]),\n            \"boundary_condition\": bc_term(xs[\"bc\"]),\n        }\n\ndef loss_fn(\n    model: PhysicsInformedNN,\n    xs: dict[str, torch.Tensor]\n) -&gt; tuple[torch.Tensor, dict[str, float]:\n    pde_residual, bc_term = model(xs)\n    loss = torch.mean(torch.sum(pde_residual**2, dim=1), dim=0)\n        + torch.mean(torch.sum(bc_term**2, dim=1), dim=0)\n\n    return loss, {\"pde_residual\": pde_residual, \"bc_term\": bc_term}\n\nxs = {\n    \"pde\": torch.linspace(0, 1, 10),\n    \"bc\": torch.tensor([0.0]),\n}\n\ninfo_landscape = InfoLandscape(model, loss_fn, xs)\n</code></pre></p> <pre><code>In this example, the model is a Physics Informed Neural Network, and the `xs`\nare the inputs to the different residual components of the model. The logic\nfor calculation of the residuals lies within the PhysicsInformedNN class, and\nthe loss function is defined to calculate the loss that is to be optimized\nfrom these residuals. This can then further be used to obtain the\nbounds on the average norm of the gradient of the loss function.\n</code></pre> <p>The first value that the <code>loss_fn</code> returns is the loss value that is being optimized. The function is also expected to return other value(s), often the metrics that are used to calculate the loss. These values are ignored for the purpose of this class.</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss_fn: Callable,\n    xs: Any,\n    epsilons: torch.Tensor,\n    variation_multiple: int = 20,\n) -&gt; None:\n    \"\"\"Information Landscape class.\n\n    This class handles the study of loss landscape from information theoretic\n    perspective and provides methods to get bounds on the norm of the\n    gradient from the Information Content of the loss landscape.\n\n    Args:\n        model: The quantum or classical model to analyze.\n        loss_fn: Loss function that takes model output and calculates loss\n        xs: Input data to evaluate the model on\n        epsilons: The thresholds to use for discretization of the finite derivatives\n        variation_multiple: The number of sets of variational parameters to generate per each\n            variational parameter. The number of variational parameters required for the\n            statistical analysis scales linearly with the amount of them present in the\n            model. This is that linear factor.\n\n    Notes:\n        This class provides flexibility in terms of what the model, the loss function,\n        and the xs are. The only requirement is that the loss_fn takes the model and xs as\n        arguments and returns the loss, and another dictionary of other metrics.\n\n        Thus, assumed structure:\n            loss_fn(model, xs) -&gt; (loss, metrics, ...)\n\n        Example: A Classifier\n            ```python\n            model = nn.Linear(10, 1)\n\n            def loss_fn(\n                model: nn.Module,\n                xs: tuple[torch.Tensor, torch.Tensor]\n            ) -&gt; tuple[torch.Tensor, dict[str, float]:\n                criterion = nn.MSELoss()\n                inputs, labels = xs\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                metrics = {\"loss\": loss.item()}\n                return loss, metrics\n\n            xs = (torch.randn(10, 10), torch.randn(10, 1))\n\n            info_landscape = InfoLandscape(model, loss_fn, xs)\n            ```\n            In this example, the model is a linear classifier, and the `xs` include both the\n            inputs and the target labels. The logic for calculation of the loss from this lies\n            entirely within the `loss_fn` function. This can then further be used to obtain the\n            bounds on the average norm of the gradient of the loss function.\n\n        Example: A Physics Informed Neural Network\n            ```python\n            class PhysicsInformedNN(nn.Module):\n                // &lt;Initialization Logic&gt;\n\n                def forward(self, xs: dict[str, torch.Tensor]):\n                    return {\n                        \"pde_residual\": pde_residual(xs[\"pde\"]),\n                        \"boundary_condition\": bc_term(xs[\"bc\"]),\n                    }\n\n            def loss_fn(\n                model: PhysicsInformedNN,\n                xs: dict[str, torch.Tensor]\n            ) -&gt; tuple[torch.Tensor, dict[str, float]:\n                pde_residual, bc_term = model(xs)\n                loss = torch.mean(torch.sum(pde_residual**2, dim=1), dim=0)\n                    + torch.mean(torch.sum(bc_term**2, dim=1), dim=0)\n\n                return loss, {\"pde_residual\": pde_residual, \"bc_term\": bc_term}\n\n            xs = {\n                \"pde\": torch.linspace(0, 1, 10),\n                \"bc\": torch.tensor([0.0]),\n            }\n\n            info_landscape = InfoLandscape(model, loss_fn, xs)\n            ```\n\n            In this example, the model is a Physics Informed Neural Network, and the `xs`\n            are the inputs to the different residual components of the model. The logic\n            for calculation of the residuals lies within the PhysicsInformedNN class, and\n            the loss function is defined to calculate the loss that is to be optimized\n            from these residuals. This can then further be used to obtain the\n            bounds on the average norm of the gradient of the loss function.\n\n        The first value that the `loss_fn` returns is the loss value that is being optimized.\n        The function is also expected to return other value(s), often the metrics that are\n        used to calculate the loss. These values are ignored for the purpose of this class.\n    \"\"\"\n    self.model = model\n    self.loss_fn = loss_fn\n    self.xs = xs\n    self.epsilons = epsilons\n    self.device = next(model.parameters()).device\n\n    self.param_shapes = {}\n    self.total_params = 0\n\n    for name, param in model.named_parameters():\n        self.param_shapes[name] = param.shape\n        self.total_params += param.numel()\n    self.n_variations = variation_multiple * self.total_params\n    self.all_variations = torch.empty(\n        (self.n_variations, self.total_params), device=self.device\n    ).uniform_(0, 2 * torch.pi)\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.calculate_IC","title":"<code>calculate_IC</code>  <code>cached</code> <code>property</code>","text":"<p>Calculate Information Content for multiple epsilon values.</p> <p>Returns: Tensor of IC values for each epsilon [n_epsilons]</p>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.batched_loss","title":"<code>batched_loss()</code>","text":"<p>Calculate loss for all parameter variations in a batched manner.</p> <p>Returns: Tensor of loss values for each parameter variation</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def batched_loss(self) -&gt; torch.Tensor:\n    \"\"\"Calculate loss for all parameter variations in a batched manner.\n\n    Returns: Tensor of loss values for each parameter variation\n    \"\"\"\n    param_variations = self.reshape_param_variations()\n    losses = torch.zeros(self.n_variations, device=self.device)\n\n    for i in range(self.n_variations):\n        params = {name: param[i] for name, param in param_variations.items()}\n        current_model = lambda x: functional_call(self.model, params, (x,))\n        losses[i] = self.loss_fn(current_model, self.xs)[0]\n\n    return losses\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.calculate_transition_probabilities_batch","title":"<code>calculate_transition_probabilities_batch()</code>","text":"<p>Calculate transition probabilities for multiple epsilon values.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of shape [n_epsilons, 6] containing probabilities for each transition type</p> <code>Tensor</code> <p>Columns order: [+1to0, +1to-1, 0to+1, 0to-1, -1to0, -1to+1]</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def calculate_transition_probabilities_batch(self) -&gt; torch.Tensor:\n    \"\"\"\n    Calculate transition probabilities for multiple epsilon values.\n\n    Returns:\n        Tensor of shape [n_epsilons, 6] containing probabilities for each transition type\n        Columns order: [+1to0, +1to-1, 0to+1, 0to-1, -1to0, -1to+1]\n    \"\"\"\n    discretized = self.discretize_derivatives()\n\n    current = discretized[:, :-1]\n    next_val = discretized[:, 1:]\n\n    transitions = torch.stack(\n        [\n            ((current == 1) &amp; (next_val == 0)).sum(dim=1),\n            ((current == 1) &amp; (next_val == -1)).sum(dim=1),\n            ((current == 0) &amp; (next_val == 1)).sum(dim=1),\n            ((current == 0) &amp; (next_val == -1)).sum(dim=1),\n            ((current == -1) &amp; (next_val == 0)).sum(dim=1),\n            ((current == -1) &amp; (next_val == 1)).sum(dim=1),\n        ],\n        dim=1,\n    ).float()\n\n    total_transitions = current.size(1)\n    probabilities = transitions / total_transitions\n\n    return probabilities\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.discretize_derivatives","title":"<code>discretize_derivatives()</code>","text":"<p>Convert finite derivatives into discrete values.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor containing discretized derivatives with shape [n_epsilons, n_variations-2]</p> <code>Tensor</code> <p>Each row contains {-1, 0, 1} values for that epsilon</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def discretize_derivatives(self) -&gt; torch.Tensor:\n    \"\"\"\n    Convert finite derivatives into discrete values.\n\n    Returns:\n        Tensor containing discretized derivatives with shape [n_epsilons, n_variations-2]\n        Each row contains {-1, 0, 1} values for that epsilon\n    \"\"\"\n    derivatives = self.randomized_finite_der()\n\n    derivatives = derivatives.unsqueeze(0)\n    epsilons = self.epsilons.unsqueeze(1)\n\n    discretized = torch.zeros((len(epsilons), len(derivatives[0])), device=self.device)\n    discretized[derivatives &gt; epsilons] = 1\n    discretized[derivatives &lt; -epsilons] = -1\n\n    return discretized\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.get_grad_norm_bounds_max_IC","title":"<code>get_grad_norm_bounds_max_IC()</code>","text":"<p>Compute the bounds on the average norm of the gradient.</p> RETURNS DESCRIPTION <code>tuple[float, float]</code> <p>tuple[Tensor, Tensor]: The lower and upper bounds.</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def get_grad_norm_bounds_max_IC(self) -&gt; tuple[float, float]:\n    \"\"\"\n    Compute the bounds on the average norm of the gradient.\n\n    Returns:\n        tuple[Tensor, Tensor]: The lower and upper bounds.\n    \"\"\"\n    max_IC, epsilon_m = self.max_IC()\n    lower_bound = (\n        epsilon_m\n        * sqrt(self.total_params)\n        / (NormalDist().inv_cdf(1 - 2 * self.q_value(max_IC)))\n    )\n    upper_bound = (\n        epsilon_m\n        * sqrt(self.total_params)\n        / (NormalDist().inv_cdf(0.5 * (1 + 2 * self.q_value(max_IC))))\n    )\n\n    if max_IC &lt; log(2, 6):\n        logger.warning(\n            \"Warning: The maximum IC is less than the required value. The bounds may be\"\n            + \" inaccurate.\"\n        )\n\n    return lower_bound, upper_bound\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.get_grad_norm_bounds_sensitivity_IC","title":"<code>get_grad_norm_bounds_sensitivity_IC(eta)</code>","text":"<p>Compute the bounds on the average norm of the gradient.</p> PARAMETER DESCRIPTION <code>eta</code> <p>The sensitivity IC.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The lower bound.</p> <p> TYPE: <code>float</code> </p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def get_grad_norm_bounds_sensitivity_IC(self, eta: float) -&gt; float:\n    \"\"\"\n    Compute the bounds on the average norm of the gradient.\n\n    Args:\n        eta (float): The sensitivity IC.\n\n    Returns:\n        Tensor: The lower bound.\n    \"\"\"\n    epsilon_sensitivity = self.sensitivity_IC(eta)\n    upper_bound = (\n        epsilon_sensitivity * sqrt(self.total_params) / (NormalDist().inv_cdf(1 - 3 * eta / 2))\n    )\n    return upper_bound\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.max_IC","title":"<code>max_IC()</code>","text":"<p>Get the maximum Information Content and its corresponding epsilon.</p> <p>Returns: Tuple of (maximum IC value, optimal epsilon)</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def max_IC(self) -&gt; tuple[float, float]:\n    \"\"\"\n    Get the maximum Information Content and its corresponding epsilon.\n\n    Returns: Tuple of (maximum IC value, optimal epsilon)\n    \"\"\"\n    max_ic, max_idx = torch.max(self.calculate_IC, dim=0)\n    max_epsilon = self.epsilons[max_idx]\n    return max_ic.item(), max_epsilon.item()\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.q_value","title":"<code>q_value(H_value)</code>  <code>cached</code> <code>staticmethod</code>","text":"<p>Compute the q value.</p> <p>q is the solution to the equation: H(x) = 4h(x) + 2h(1/2 - 2x)</p> <p>It is the value of the probability of 4 of the 6 transitions such that the IC is the same as the IC of our system.</p> <p>This quantity is useful in calculating the bounds on the norms of the gradients.</p> PARAMETER DESCRIPTION <code>H_value</code> <p>The information content.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The q value</p> <p> TYPE: <code>float</code> </p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>@staticmethod\n@functools.lru_cache\ndef q_value(H_value: float) -&gt; float:\n    \"\"\"\n    Compute the q value.\n\n    q is the solution to the equation:\n    H(x) = 4h(x) + 2h(1/2 - 2x)\n\n    It is the value of the probability of 4 of the 6 transitions such that\n    the IC is the same as the IC of our system.\n\n    This quantity is useful in calculating the bounds on the norms of the gradients.\n\n    Args:\n        H_value (float): The information content.\n\n    Returns:\n        float: The q value\n    \"\"\"\n\n    x = torch.linspace(0.001, 0.16667, 10000)\n\n    H = -4 * x * torch.log(x) / torch.log(torch.tensor(6)) - 2 * (0.5 - 2 * x) * torch.log(\n        0.5 - 2 * x\n    ) / torch.log(torch.tensor(6))\n    err = torch.abs(H - H_value)\n    idx = torch.argmin(err)\n    return float(x[idx].item())\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.randomized_finite_der","title":"<code>randomized_finite_der()</code>","text":"<p>Calculate normalized finite difference of loss on doing random walk in the parameter space.</p> <p>This serves as a proxy for the derivative of the loss with respect to parameters.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor containing normalized finite differences (approximate directional derivatives)</p> <code>Tensor</code> <p>between consecutive points in the random walk. Shape: [n_variations - 1]</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def randomized_finite_der(self) -&gt; torch.Tensor:\n    \"\"\"\n    Calculate normalized finite difference of loss on doing random walk in the parameter space.\n\n    This serves as a proxy for the derivative of the loss with respect to parameters.\n\n    Returns:\n        Tensor containing normalized finite differences (approximate directional derivatives)\n        between consecutive points in the random walk. Shape: [n_variations - 1]\n    \"\"\"\n    losses = self.batched_loss()\n\n    return (losses[1:] - losses[:-1]) / (\n        torch.norm(self.all_variations[1:] - self.all_variations[:-1], dim=1) + 1e-8\n    )\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.reshape_param_variations","title":"<code>reshape_param_variations()</code>","text":"<p>Reshape variations of the model's variational parameters.</p> RETURNS DESCRIPTION <code>dict[str, Tensor]</code> <p>Dictionary of parameter tensors, each with shape [n_variations, *param_shape]</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def reshape_param_variations(self) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Reshape variations of the model's variational parameters.\n\n    Returns:\n        Dictionary of parameter tensors, each with shape [n_variations, *param_shape]\n    \"\"\"\n    param_variations = {}\n    start_idx = 0\n\n    for name, shape in self.param_shapes.items():\n        param_size = torch.prod(torch.tensor(shape)).item()\n        param_variations[name] = self.all_variations[\n            :, start_idx : start_idx + param_size\n        ].view(self.n_variations, *shape)\n        start_idx += param_size\n\n    return param_variations\n</code></pre>"},{"location":"api/information/#perceptrain.information.information_content.InformationContent.sensitivity_IC","title":"<code>sensitivity_IC(eta)</code>","text":"<p>Find the minimum value of epsilon such that the information content is less than eta.</p> PARAMETER DESCRIPTION <code>eta</code> <p>Threshold value, the sensitivity IC.</p> <p> TYPE: <code>float</code> </p> <p>Returns: The epsilon value that gives IC that is less than the sensitivity IC.</p> Source code in <code>perceptrain/information/information_content.py</code> <pre><code>def sensitivity_IC(self, eta: float) -&gt; float:\n    \"\"\"\n    Find the minimum value of epsilon such that the information content is less than eta.\n\n    Args:\n        eta: Threshold value, the sensitivity IC.\n\n    Returns: The epsilon value that gives IC that is less than the sensitivity IC.\n    \"\"\"\n    ic_values = self.calculate_IC\n    mask = ic_values &lt; eta\n    epsilons = self.epsilons[mask]\n    return float(epsilons.min().item())\n</code></pre>"},{"location":"api/trainer/","title":"Trainer","text":""},{"location":"api/trainer/#trainer","title":"Trainer","text":"<p>This is the API for <code>Trainer</code> class.</p>"},{"location":"api/trainer/#perceptrain.trainer.Trainer","title":"<code>Trainer(model, optimizer, config, loss_fn='mse', train_dataloader=None, val_dataloader=None, test_dataloader=None, optimize_step=optimize_step, max_batches=None)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class to manage and execute training, validation, and testing loops for a model (eg.</p> <p>QNN).</p> <p>This class handles the overall training process, including: - Managing epochs and steps - Handling data loading and batching - Computing and updating gradients - Logging and monitoring training metrics</p> ATTRIBUTE DESCRIPTION <code>current_epoch</code> <p>The current epoch number.</p> <p> TYPE: <code>int</code> </p> <code>global_step</code> <p>The global step across all epochs.</p> <p> TYPE: <code>int</code> </p> Inherited Attributes <p>use_grad (bool): Indicates if gradients are used for optimization. Default is True.</p> <p>model (nn.Module): The neural network model. optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training. config (TrainConfig): The configuration settings for training. train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data. val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data. test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.</p> <p>optimize_step (Callable): Function for performing an optimization step. loss_fn (Callable): loss function to use.</p> <p>num_training_batches (int): Number of training batches. num_validation_batches (int): Number of validation batches. num_test_batches (int): Number of test batches.</p> <p>state (str): Current state in the training process</p> <p>Default training routine <pre><code>for epoch in max_iter + 1:\n    # Training\n    for batch in train_batches:\n        train model\n    # Validation\n    if val_every % epoch == 0:\n        for batch in val_batches:\n            train model\n</code></pre></p> Notes <ul> <li>In case of InfiniteTensorDataset, number of batches = 1.</li> <li>In case of TensorDataset, number of batches are default.</li> <li>Training is run for max_iter + 1 epochs. Epoch 0 logs untrained model.</li> <li>Please look at the CallbackManager initialize_callbacks method to review the default     logging behavior.</li> </ul> <p>Examples: IMP: This uses qadence models (QNN), and should be used carefully. <pre><code>import torch\nfrom torch.optim import SGD\nfrom perceptrain import (\n    feature_map,\n    hamiltonian_factory,\n    hea,\n    QNN,\n    QuantumCircuit,\n    TrainConfig,\n    Z,\n)\nfrom perceptrain.trainer import Trainer\nfrom perceptrain.optimize_step import optimize_step\nfrom perceptrain import TrainConfig\nfrom perceptrain.data import to_dataloader\n\n# Initialize the model\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=2)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\n\n# Set up the optimizer\noptimizer = SGD(model.parameters(), lr=0.001)\n\n# Use TrainConfig for configuring the training process\nconfig = TrainConfig(\n    max_iter=100,\n    print_every=10,\n    write_every=10,\n    checkpoint_every=10,\n    val_every=10\n)\n\n# Create the Trainer instance with TrainConfig\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\",\n    optimize_step=optimize_step\n)\n\nbatch_size = 25\nx = torch.linspace(0, 1, 32).reshape(-1, 1)\ny = torch.sin(x)\ntrain_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_loader = to_dataloader(x, y, batch_size=batch_size, infinite=False)\n\n# Train the model\nmodel, optimizer = trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>This also supports both gradient based and gradient free optimization. The default support is for gradient based optimization.</p> <p>Notes:</p> <ul> <li>set_use_grad() (class level):This method is used to set the global <code>use_grad</code> flag,     controlling whether the trainer uses gradient-based optimization. <pre><code># gradient based\nTrainer.set_use_grad(True)\n\n# gradient free\nTrainer.set_use_grad(False)\n</code></pre></li> <li>Context Managers (instance level):  <code>enable_grad_opt()</code> and <code>disable_grad_opt()</code> are     context managers that temporarily switch the optimization mode for specific code blocks.     This is useful when you want to mix gradient-based and gradient-free optimization     in the same training process. <pre><code># gradient based\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit()\n\n# gradient free\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit()\n</code></pre></li> </ul> <p>Examples</p> <p>Gradient based optimization example Usage: <pre><code>from torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nTrainer.set_use_grad(True)\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>trainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Gradient free optimization example Usage: <pre><code>import nevergrad as ng\nfrom perceptrain.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n                budget=config.max_iter, parametrization= num_parameters(model)\n                )\n\nTrainer.set_use_grad(False)\ntrainer = Trainer(\n    model=model,\n    optimizer=ng_optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>import nevergrad as ng\nfrom perceptrain.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n        budget=config.max_iter, parametrization= num_parameters(model)\n        )\n\ntrainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Initializes the Trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>Training configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>Loss function used for training. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>optimize_step</code> <p>Function to execute an optimization step.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>optimize_step</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    optimize_step: Callable = optimize_step,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the Trainer class.\n\n    Args:\n        model (nn.Module): The PyTorch model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training.\n        config (TrainConfig): Training configuration object.\n        loss_fn (str | Callable ): Loss function used for training.\n            If not specified, default mse loss will be used.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for test data.\n        optimize_step (Callable): Function to execute an optimization step.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    super().__init__(\n        model=model,\n        optimizer=optimizer,\n        config=config,\n        loss_fn=loss_fn,\n        optimize_step=optimize_step,\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        test_dataloader=test_dataloader,\n        max_batches=max_batches,\n    )\n    self.current_epoch: int = 0\n    self.global_step: int = 0\n    self._stop_training: torch.Tensor = torch.tensor(0, dtype=torch.int)\n    self.progress: Progress | None = None\n\n    # Integration with Accelerator:\n    self.accelerator = Accelerator(\n        backend=config.backend,\n        nprocs=config.nprocs,\n        compute_setup=config.compute_setup,\n        dtype=config.dtype,\n        log_setup=config.log_setup,\n    )\n    # Decorate the unbound Trainer.fit method with accelerator.distribute.\n    # We use __get__ to bind the decorated method to the current instance,\n    # ensuring that 'self' is passed only once when self.fit is called.\n    self.fit = self.accelerator.distribute(Trainer.fit).__get__(self, Trainer)  # type: ignore[method-assign]\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.build_optimize_result","title":"<code>build_optimize_result(result)</code>","text":"<p>Builds and stores the optimization result by calculating the average loss and metrics.</p> <p>Result (or loss_metrics) can have multiple formats: - <code>None</code> Indicates no loss or metrics data is provided. - <code>tuple[torch.Tensor, dict[str, Any]]</code> A single tuple containing the loss tensor     and metrics dictionary - at the end of batch. - <code>list[tuple[torch.Tensor, dict[str, Any]]]</code> A list of tuples for     multiple batches. - <code>list[list[tuple[torch.Tensor, dict[str, Any]]]]</code> A list of lists of tuples, where each inner list represents metrics across multiple batches within an epoch.</p> PARAMETER DESCRIPTION <code>result</code> <p>(None |     tuple[torch.Tensor, dict[Any, Any]] |     list[tuple[torch.Tensor, dict[Any, Any]]] |     list[list[tuple[torch.Tensor, dict[Any, Any]]]])         The loss and metrics data, which can have multiple formats</p> <p> TYPE: <code>None | tuple[Tensor, dict[Any, Any]] | list[tuple[Tensor, dict[Any, Any]]] | list[list[tuple[Tensor, dict[Any, Any]]]]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>This method does not return anything. It sets <code>self.opt_result</code> with</p> <p> TYPE: <code>None</code> </p> <code>None</code> <p>the computed average loss and metrics.</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>def build_optimize_result(\n    self,\n    result: (\n        None\n        | tuple[torch.Tensor, dict[Any, Any]]\n        | list[tuple[torch.Tensor, dict[Any, Any]]]\n        | list[list[tuple[torch.Tensor, dict[Any, Any]]]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Builds and stores the optimization result by calculating the average loss and metrics.\n\n    Result (or loss_metrics) can have multiple formats:\n    - `None` Indicates no loss or metrics data is provided.\n    - `tuple[torch.Tensor, dict[str, Any]]` A single tuple containing the loss tensor\n        and metrics dictionary - at the end of batch.\n    - `list[tuple[torch.Tensor, dict[str, Any]]]` A list of tuples for\n        multiple batches.\n    - `list[list[tuple[torch.Tensor, dict[str, Any]]]]` A list of lists of tuples,\n    where each inner list represents metrics across multiple batches within an epoch.\n\n    Args:\n        result: (None |\n                tuple[torch.Tensor, dict[Any, Any]] |\n                list[tuple[torch.Tensor, dict[Any, Any]]] |\n                list[list[tuple[torch.Tensor, dict[Any, Any]]]])\n                    The loss and metrics data, which can have multiple formats\n\n    Returns:\n        None: This method does not return anything. It sets `self.opt_result` with\n        the computed average loss and metrics.\n    \"\"\"\n    loss_metrics = result\n    if loss_metrics is None:\n        loss = None\n        metrics: dict[Any, Any] = {}\n    elif isinstance(loss_metrics, tuple):\n        # Single tuple case\n        loss, metrics = loss_metrics\n    else:\n        last_epoch: list[tuple[torch.Tensor, dict[Any, Any]]] = []\n        if isinstance(loss_metrics, list):\n            # Check if it's a list of tuples\n            if all(isinstance(item, tuple) for item in loss_metrics):\n                last_epoch = cast(list[tuple[torch.Tensor, dict[Any, Any]]], loss_metrics)\n            # Check if it's a list of lists of tuples\n            elif all(isinstance(item, list) for item in loss_metrics):\n                last_epoch = cast(\n                    list[tuple[torch.Tensor, dict[Any, Any]]],\n                    loss_metrics[-1] if loss_metrics else [],\n                )\n            else:\n                raise ValueError(\n                    \"Invalid format for result: Expected None, tuple, list of tuples,\"\n                    \" or list of lists of tuples.\"\n                )\n\n        if not last_epoch:\n            loss, metrics = None, {}\n        else:\n            # Compute the average loss over the batches\n            loss_tensor = torch.stack([loss_batch for loss_batch, _ in last_epoch])\n            avg_loss = loss_tensor.mean()\n\n            # Collect and average metrics for all batches\n            metric_keys = last_epoch[0][1].keys()\n            metrics_stacked: dict = {key: [] for key in metric_keys}\n\n            for _, metrics_batch in last_epoch:\n                for key in metric_keys:\n                    value = metrics_batch[key]\n                    metrics_stacked[key].append(value)\n\n            avg_metrics = {key: torch.stack(metrics_stacked[key]).mean() for key in metric_keys}\n\n            loss, metrics = avg_loss, avg_metrics\n\n    # Store the optimization result\n    self.opt_result = OptimizeResult(\n        self.current_epoch,\n        self.model,\n        self.optimizer,\n        loss,\n        metrics,\n        rank=self.accelerator.rank,\n        device=self.accelerator.execution.device,\n    )\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.fit","title":"<code>fit(train_dataloader=None, val_dataloader=None)</code>","text":"<p>Fits the model using the specified training configuration.</p> <p>The dataloaders can be provided to train on new datasets, or the default dataloaders provided in the trainer will be used.</p> PARAMETER DESCRIPTION <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Module, Optimizer]</code> <p>tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>def fit(\n    self,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; tuple[nn.Module, optim.Optimizer]:\n    \"\"\"\n    Fits the model using the specified training configuration.\n\n    The dataloaders can be provided to train on new datasets, or the default dataloaders\n    provided in the trainer will be used.\n\n    Args:\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n\n    Returns:\n        tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.\n    \"\"\"\n    if train_dataloader is not None:\n        self.train_dataloader = train_dataloader\n    if val_dataloader is not None:\n        self.val_dataloader = val_dataloader\n\n    self._fit_setup()\n    self._train()\n    self._fit_end()\n    self.training_stage = TrainingStage(\"idle\")\n    return self.model, self.optimizer\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.get_ic_grad_bounds","title":"<code>get_ic_grad_bounds(eta, epsilons, variation_multiple=20, dataloader=None)</code>","text":"<p>Calculate the bounds on the gradient norm of the loss using Information Content.</p> PARAMETER DESCRIPTION <code>eta</code> <p>The sensitivity IC.</p> <p> TYPE: <code>float</code> </p> <code>epsilons</code> <p>The epsilons to use for thresholds to for discretization of the finite derivatives.</p> <p> TYPE: <code>Tensor</code> </p> <code>variation_multiple</code> <p>The number of sets of variational parameters to generate per each variational parameter. The number of variational parameters required for the statisctiacal analysis scales linearly with the amount of them present in the model. This is that linear factor.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>dataloader</code> <p>The dataloader for training data. A new dataloader can be provided, or the dataloader provided in the trinaer will be used. In case no dataloaders are provided at either places, it assumes that the model does not require any input data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[float, float, float]</code> <p>tuple[float, float, float]: The max IC lower bound, max IC upper bound, and sensitivity IC upper bound.</p> <p>Examples:</p> <pre><code>import torch\nfrom torch.optim.adam import Adam\n\nfrom perceptrain.constructors import ObservableConfig\nfrom perceptrain.config import AnsatzConfig, FeatureMapConfig, TrainConfig\nfrom perceptrain.data import to_dataloader\nfrom perceptrain import QNN\nfrom perceptrain.optimize_step import optimize_step\nfrom perceptrain.trainer import Trainer\nfrom perceptrain.operations.primitive import Z\n\nfm_config = FeatureMapConfig(num_features=1)\nansatz_config = AnsatzConfig(depth=4)\nobs_config = ObservableConfig(detuning=Z)\n\nqnn = QNN.from_configs(\n    register=4,\n    obs_config=obs_config,\n    fm_config=fm_config,\n    ansatz_config=ansatz_config,\n)\n\noptimizer = Adam(qnn.parameters(), lr=0.001)\n\nbatch_size = 25\nx = torch.linspace(0, 1, 32).reshape(-1, 1)\ny = torch.sin(x)\ntrain_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\ntrain_config = TrainConfig(max_iter=100)\n\ntrainer = Trainer(\n    model=qnn,\n    optimizer=optimizer,\n    config=train_config,\n    loss_fn=\"mse\",\n    train_dataloader=train_loader,\n    optimize_step=optimize_step,\n)\n\n# Perform exploratory landscape analysis with Information Content\nic_sensitivity_threshold = 1e-4\nepsilons = torch.logspace(-2, 2, 10)\n\nmax_ic_lower_bound, max_ic_upper_bound, sensitivity_ic_upper_bound = (\n    trainer.get_ic_grad_bounds(\n        eta=ic_sensitivity_threshold,\n        epsilons=epsilons,\n    )\n)\n\n# Resume training as usual...\n\ntrainer.fit(train_loader)\n</code></pre> Source code in <code>perceptrain/trainer.py</code> <pre><code>def get_ic_grad_bounds(\n    self,\n    eta: float,\n    epsilons: torch.Tensor,\n    variation_multiple: int = 20,\n    dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; tuple[float, float, float]:\n    \"\"\"\n    Calculate the bounds on the gradient norm of the loss using Information Content.\n\n    Args:\n        eta (float): The sensitivity IC.\n        epsilons (torch.Tensor): The epsilons to use for thresholds to for discretization of the\n            finite derivatives.\n        variation_multiple (int): The number of sets of variational parameters to generate per\n            each variational parameter. The number of variational parameters required for the\n            statisctiacal analysis scales linearly with the amount of them present in the\n            model. This is that linear factor.\n        dataloader (DataLoader | DictDataLoader | None): The dataloader for training data. A\n            new dataloader can be provided, or the dataloader provided in the trinaer will be\n            used. In case no dataloaders are provided at either places, it assumes that the\n            model does not require any input data.\n\n    Returns:\n        tuple[float, float, float]: The max IC lower bound, max IC upper bound, and sensitivity\n            IC upper bound.\n\n    Examples:\n        ```python\n        import torch\n        from torch.optim.adam import Adam\n\n        from perceptrain.constructors import ObservableConfig\n        from perceptrain.config import AnsatzConfig, FeatureMapConfig, TrainConfig\n        from perceptrain.data import to_dataloader\n        from perceptrain import QNN\n        from perceptrain.optimize_step import optimize_step\n        from perceptrain.trainer import Trainer\n        from perceptrain.operations.primitive import Z\n\n        fm_config = FeatureMapConfig(num_features=1)\n        ansatz_config = AnsatzConfig(depth=4)\n        obs_config = ObservableConfig(detuning=Z)\n\n        qnn = QNN.from_configs(\n            register=4,\n            obs_config=obs_config,\n            fm_config=fm_config,\n            ansatz_config=ansatz_config,\n        )\n\n        optimizer = Adam(qnn.parameters(), lr=0.001)\n\n        batch_size = 25\n        x = torch.linspace(0, 1, 32).reshape(-1, 1)\n        y = torch.sin(x)\n        train_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\n        train_config = TrainConfig(max_iter=100)\n\n        trainer = Trainer(\n            model=qnn,\n            optimizer=optimizer,\n            config=train_config,\n            loss_fn=\"mse\",\n            train_dataloader=train_loader,\n            optimize_step=optimize_step,\n        )\n\n        # Perform exploratory landscape analysis with Information Content\n        ic_sensitivity_threshold = 1e-4\n        epsilons = torch.logspace(-2, 2, 10)\n\n        max_ic_lower_bound, max_ic_upper_bound, sensitivity_ic_upper_bound = (\n            trainer.get_ic_grad_bounds(\n                eta=ic_sensitivity_threshold,\n                epsilons=epsilons,\n            )\n        )\n\n        # Resume training as usual...\n\n        trainer.fit(train_loader)\n        ```\n    \"\"\"\n    if not self._use_grad:\n        logger.warning(\n            \"Gradient norm bounds are only relevant when using a gradient based optimizer. \\\n                Currently the trainer is set to use a gradient-free optimizer.\"\n        )\n\n    dataloader = dataloader if dataloader is not None else self.train_dataloader\n\n    batch = next(iter(self._batch_iter(dataloader, num_batches=1)))\n\n    ic = InformationContent(self.model, self.loss_fn, batch, epsilons)\n\n    max_ic_lower_bound, max_ic_upper_bound = ic.get_grad_norm_bounds_max_IC()\n    sensitivity_ic_upper_bound = ic.get_grad_norm_bounds_sensitivity_IC(eta)\n\n    return max_ic_lower_bound, max_ic_upper_bound, sensitivity_ic_upper_bound\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.run_test_batch","title":"<code>run_test_batch(batch)</code>","text":"<p>Runs a single test batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>@BaseTrainer.callback(\"test_batch\")\ndef run_test_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single test batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.run_train_batch","title":"<code>run_train_batch(batch)</code>","text":"<p>Runs a single training batch, performing optimization.</p> <p>We use the step function to optimize the model based on use_grad.     use_grad = True entails gradient based optimization, for which we use     optimize_step function.     use_grad = False entails gradient free optimization, for which we use     update_ng_parameters function.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch. tuple of (loss, metrics)</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_batch\")\ndef run_train_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single training batch, performing optimization.\n\n    We use the step function to optimize the model based on use_grad.\n        use_grad = True entails gradient based optimization, for which we use\n        optimize_step function.\n        use_grad = False entails gradient free optimization, for which we use\n        update_ng_parameters function.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n            tuple of (loss, metrics)\n    \"\"\"\n\n    if self.use_grad:\n        # Perform gradient-based optimization\n        loss_metrics = self.optimize_step(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            xs=batch,\n            device=self.accelerator.execution.device,\n            dtype=self.accelerator.execution.data_dtype,\n        )\n    else:\n        # Perform optimization using Nevergrad\n        loss, metrics, ng_params = update_ng_parameters(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            data=batch,\n            ng_params=self.ng_params,  # type: ignore[arg-type]\n        )\n        self.ng_params = ng_params\n        loss_metrics = loss, metrics\n\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.run_training","title":"<code>run_training(dataloader)</code>","text":"<p>Runs the training for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_epoch\")\ndef run_training(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the training for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for training data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.train()\n    train_epoch_loss_metrics = []\n\n    for batch in self._batch_iter(dataloader, self.num_training_batches):\n        self.on_train_batch_start(batch)\n        train_batch_loss_metrics = self.run_train_batch(batch)\n        if self.config.all_reduce_metrics:\n            train_batch_loss_metrics = self._aggregate_result(train_batch_loss_metrics)\n        train_epoch_loss_metrics.append(train_batch_loss_metrics)\n        self.on_train_batch_end(train_batch_loss_metrics)\n\n    return train_epoch_loss_metrics\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.run_val_batch","title":"<code>run_val_batch(batch)</code>","text":"<p>Runs a single validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_batch\")\ndef run_val_batch(self, batch: tuple[torch.Tensor, ...]) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single validation batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.run_validation","title":"<code>run_validation(dataloader)</code>","text":"<p>Runs the validation loop for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_epoch\")\ndef run_validation(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the validation loop for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for validation data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.eval()\n    val_epoch_loss_metrics = []\n\n    for batch in self._batch_iter(dataloader, self.num_validation_batches):\n        self.on_val_batch_start(batch)\n        val_batch_loss_metrics = self.run_val_batch(batch)\n        if self.config.all_reduce_metrics:\n            val_batch_loss_metrics = self._aggregate_result(val_batch_loss_metrics)\n        val_epoch_loss_metrics.append(val_batch_loss_metrics)\n        self.on_val_batch_end(val_batch_loss_metrics)\n\n    return val_epoch_loss_metrics\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.stop_training","title":"<code>stop_training()</code>","text":"<p>Helper function to indicate if the training should be stopped.</p> <p>We all_reduce the indicator across all processes to ensure all processes are stopped.</p> Notes <p>self._stop_training indicator indicates if the training should be stopped. 0 is continue. 1 is stop.</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>def stop_training(self) -&gt; bool:\n    \"\"\"\n    Helper function to indicate if the training should be stopped.\n\n    We all_reduce the indicator across all processes to ensure all processes are stopped.\n\n    Notes:\n        self._stop_training indicator indicates if the training should be stopped.\n        0 is continue. 1 is stop.\n    \"\"\"\n    _stop_training = self.accelerator.all_reduce_dict(\n        {\"indicator\": self._stop_training}, op=\"max\"\n    )\n    return bool(_stop_training[\"indicator\"] &gt; 0)\n</code></pre>"},{"location":"api/trainer/#perceptrain.trainer.Trainer.test","title":"<code>test(test_dataloader=None)</code>","text":"<p>Runs the testing loop if a test DataLoader is provided.</p> <p>if the test_dataloader is not provided, default test_dataloader defined in the Trainer class is used.</p> PARAMETER DESCRIPTION <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                    -&gt; tuples Test Batches            -&gt; (loss, metrics)</p> Source code in <code>perceptrain/trainer.py</code> <pre><code>def test(self, test_dataloader: DataLoader = None) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the testing loop if a test DataLoader is provided.\n\n    if the test_dataloader is not provided, default test_dataloader defined\n    in the Trainer class is used.\n\n    Args:\n        test_dataloader (DataLoader): DataLoader for test data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                    -&gt; tuples\n            Test Batches            -&gt; (loss, metrics)\n    \"\"\"\n    if test_dataloader is not None:\n        self.test_dataloader = test_dataloader\n\n    self.model.eval()\n    test_loss_metrics = []\n\n    for batch in self._batch_iter(test_dataloader, self.num_training_batches):\n        self.on_test_batch_start(batch)\n        loss_metrics = self.run_test_batch(batch)\n        test_loss_metrics.append(loss_metrics)\n        self.on_test_batch_end(loss_metrics)\n\n    return test_loss_metrics\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer","title":"<code>BaseTrainer(model, optimizer, config, loss_fn='mse', optimize_step=optimize_step, train_dataloader=None, val_dataloader=None, test_dataloader=None, max_batches=None)</code>","text":"<p>Base class for training machine learning models using a given optimizer.</p> <p>The base class implements contextmanager for gradient based/free optimization, properties, property setters, input validations, callback decorator generator, and empty hooks for different training steps.</p> This class provides <ul> <li>Context managers for enabling/disabling gradient-based optimization</li> <li>Properties for managing models, optimizers, and dataloaders</li> <li>Input validations and a callback decorator generator</li> <li>Config and callback managers using the provided <code>TrainConfig</code></li> </ul> ATTRIBUTE DESCRIPTION <code>use_grad</code> <p>Indicates if gradients are used for optimization. Default is True.</p> <p> TYPE: <code>bool</code> </p> <code>model</code> <p>The neural network model.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The configuration settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>optimize_step</code> <p>Function for performing an optimization step.</p> <p> TYPE: <code>Callable</code> </p> <code>loss_fn</code> <p>loss function to use. Default loss function used is 'mse'</p> <p> TYPE: <code>Callable | str ]</code> </p> <code>num_training_batches</code> <p>Number of training batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_validation_batches</code> <p>Number of validation batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_test_batches</code> <p>Number of test batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>state</code> <p>Current state in the training process</p> <p> TYPE: <code>str</code> </p> <p>Initializes the BaseTrainer.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The TrainConfig settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>The loss function to use. str input to be specified to use a default loss function. currently supported loss functions: 'mse', 'cross_entropy'. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data. If the model does not need data to evaluate loss, no dataset should be provided.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    optimize_step: Callable = optimize_step,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the BaseTrainer.\n\n    Args:\n        model (nn.Module): The model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer\n            for training.\n        config (TrainConfig): The TrainConfig settings for training.\n        loss_fn (str | Callable): The loss function to use.\n            str input to be specified to use a default loss function.\n            currently supported loss functions: 'mse', 'cross_entropy'.\n            If not specified, default mse loss will be used.\n        train_dataloader (Dataloader | DictDataLoader | None): DataLoader for training data.\n            If the model does not need data to evaluate loss, no dataset\n            should be provided.\n        val_dataloader (Dataloader | DictDataLoader | None): DataLoader for validation data.\n        test_dataloader (Dataloader | DictDataLoader | None): DataLoader for testing data.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    self._model: nn.Module\n    self._optimizer: optim.Optimizer | NGOptimizer | None\n    self._config: TrainConfig\n    self._train_dataloader: DataLoader | DictDataLoader | None = None\n    self._val_dataloader: DataLoader | DictDataLoader | None = None\n    self._test_dataloader: DataLoader | DictDataLoader | None = None\n\n    self.config = config\n    self.model = model\n    self.optimizer = optimizer\n    self.max_batches = max_batches\n\n    self.num_training_batches: int\n    self.num_validation_batches: int\n    self.num_test_batches: int\n\n    self.train_dataloader = train_dataloader\n    self.val_dataloader = val_dataloader\n    self.test_dataloader = test_dataloader\n\n    self.loss_fn: Callable = get_loss_fn(loss_fn)\n    self.optimize_step: Callable = optimize_step\n    self.ng_params: ng.p.Array\n    self.training_stage: TrainingStage = TrainingStage(\"idle\")\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.config","title":"<code>config</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training configuration.</p> RETURNS DESCRIPTION <code>TrainConfig</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.model","title":"<code>model</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the model if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Module</code> <p>nn.Module: The model.</p>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.optimizer","title":"<code>optimizer</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimizer if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Optimizer | Optimizer | None</code> <p>optim.Optimizer | NGOptimizer | None: The optimizer.</p>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.test_dataloader","title":"<code>test_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the test DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for testing data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.train_dataloader","title":"<code>train_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.use_grad","title":"<code>use_grad</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimization framework for the trainer.</p> <p>use_grad = True : Gradient based optimization use_grad = False : Gradient free optimization</p> RETURNS DESCRIPTION <code>bool</code> <p>Bool value for using gradient.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.val_dataloader","title":"<code>val_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the validation DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.callback","title":"<code>callback(phase)</code>  <code>staticmethod</code>","text":"<p>Decorator for executing callbacks before and after a phase.</p> <p>Phase are different hooks during the training. list of valid phases is defined in Callbacks. We also update the current state of the training process in the callback decorator.</p> PARAMETER DESCRIPTION <code>phase</code> <p>The phase for which the callback is executed (e.g., \"train\", \"train_epoch\", \"train_batch\").</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>The decorated function.</p> <p> TYPE: <code>Callable</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>@staticmethod\ndef callback(phase: str) -&gt; Callable:\n    \"\"\"\n    Decorator for executing callbacks before and after a phase.\n\n    Phase are different hooks during the training. list of valid\n    phases is defined in Callbacks.\n    We also update the current state of the training process in\n    the callback decorator.\n\n    Args:\n        phase (str): The phase for which the callback is executed (e.g., \"train\",\n            \"train_epoch\", \"train_batch\").\n\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\n\n    def decorator(method: Callable) -&gt; Callable:\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -&gt; Any:\n            start_event = f\"{phase}_start\"\n            end_event = f\"{phase}_end\"\n\n            self.training_stage = TrainingStage(start_event)\n            self.callback_manager.run_callbacks(trainer=self)\n            result = method(self, *args, **kwargs)\n\n            self.training_stage = TrainingStage(end_event)\n            # build_optimize_result method is defined in the trainer.\n            self.build_optimize_result(result)\n            self.callback_manager.run_callbacks(trainer=self)\n\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.disable_grad_opt","title":"<code>disable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily disable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The Nevergrad optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef disable_grad_opt(self, optimizer: NGOptimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily disable gradient-based optimization.\n\n    Args:\n        optimizer (NGOptimizer): The Nevergrad optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = False\n        self.callback_manager.use_grad = False\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.enable_grad_opt","title":"<code>enable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily enable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The PyTorch optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef enable_grad_opt(self, optimizer: optim.Optimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily enable gradient-based optimization.\n\n    Args:\n        optimizer (optim.Optimizer): The PyTorch optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = True\n        self.callback_manager.use_grad = True\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_test_batch_end","title":"<code>on_test_batch_end(test_batch_loss_metrics)</code>","text":"<p>Called at the end of each testing batch.</p> PARAMETER DESCRIPTION <code>test_batch_loss_metrics</code> <p>Metrics for the testing batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_end(self, test_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each testing batch.\n\n    Args:\n        test_batch_loss_metrics: Metrics for the testing batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_test_batch_start","title":"<code>on_test_batch_start(batch)</code>","text":"<p>Called at the start of each testing batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each testing batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_train_batch_end","title":"<code>on_train_batch_end(train_batch_loss_metrics)</code>","text":"<p>Called at the end of each training batch.</p> PARAMETER DESCRIPTION <code>train_batch_loss_metrics</code> <p>Metrics for the training batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_end(self, train_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each training batch.\n\n    Args:\n        train_batch_loss_metrics: Metrics for the training batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_train_batch_start","title":"<code>on_train_batch_start(batch)</code>","text":"<p>Called at the start of each training batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_train_end","title":"<code>on_train_end(train_losses, val_losses=None)</code>","text":"<p>Called at the end of training.</p> PARAMETER DESCRIPTION <code>train_losses</code> <p>Metrics for the training losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]]</code> </p> <code>val_losses</code> <p>Metrics for the validation losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_train_end(\n    self,\n    train_losses: list[list[tuple[torch.Tensor, Any]]],\n    val_losses: list[list[tuple[torch.Tensor, Any]]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Called at the end of training.\n\n    Args:\n        train_losses (list[list[tuple[torch.Tensor, Any]]]):\n            Metrics for the training losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Training Batches      -&gt; (loss, metrics)\n        val_losses (list[list[tuple[torch.Tensor, Any]]] | None):\n            Metrics for the validation losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_train_epoch_end","title":"<code>on_train_epoch_end(train_epoch_loss_metrics)</code>","text":"<p>Called at the end of each training epoch.</p> PARAMETER DESCRIPTION <code>train_epoch_loss_metrics</code> <p>Metrics for the training epoch losses. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_end(self, train_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each training epoch.\n\n    Args:\n        train_epoch_loss_metrics: Metrics for the training epoch losses.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>Called at the start of each training epoch.</p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each training epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_train_start","title":"<code>on_train_start()</code>","text":"<p>Called at the start of training.</p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_train_start(self) -&gt; None:\n    \"\"\"Called at the start of training.\"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_val_batch_end","title":"<code>on_val_batch_end(val_batch_loss_metrics)</code>","text":"<p>Called at the end of each validation batch.</p> PARAMETER DESCRIPTION <code>val_batch_loss_metrics</code> <p>Metrics for the validation batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_end(self, val_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation batch.\n\n    Args:\n        val_batch_loss_metrics: Metrics for the validation batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_val_batch_start","title":"<code>on_val_batch_start(batch)</code>","text":"<p>Called at the start of each validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each validation batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_val_epoch_end","title":"<code>on_val_epoch_end(val_epoch_loss_metrics)</code>","text":"<p>Called at the end of each validation epoch.</p> PARAMETER DESCRIPTION <code>val_epoch_loss_metrics</code> <p>Metrics for the validation epoch loss. list                    -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_end(self, val_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation epoch.\n\n    Args:\n        val_epoch_loss_metrics: Metrics for the validation epoch loss.\n            list                    -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.on_val_epoch_start","title":"<code>on_val_epoch_start()</code>","text":"<p>Called at the start of each validation epoch.</p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each validation epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/trainer/#perceptrain.train_utils.base_trainer.BaseTrainer.set_use_grad","title":"<code>set_use_grad(value)</code>  <code>classmethod</code>","text":"<p>Sets the global use_grad flag.</p> PARAMETER DESCRIPTION <code>value</code> <p>Whether to use gradient-based optimization.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>perceptrain/train_utils/base_trainer.py</code> <pre><code>@classmethod\ndef set_use_grad(cls, value: bool) -&gt; None:\n    \"\"\"\n    Sets the global use_grad flag.\n\n    Args:\n        value (bool): Whether to use gradient-based optimization.\n    \"\"\"\n    if not isinstance(value, bool):\n        raise TypeError(\"use_grad must be a boolean value.\")\n    cls._use_grad = value\n</code></pre>"},{"location":"getting_started/CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"getting_started/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"getting_started/CONTRIBUTING/","title":"How to contribute","text":"<p>We're grateful for your interest in participating in Perceptrain. Please follow our guidelines to ensure a smooth contribution process.</p>"},{"location":"getting_started/CONTRIBUTING/#reporting-an-issue-or-proposing-a-feature","title":"Reporting an issue or proposing a feature","text":"<p>Your course of action will depend on your objective, but generally, you should start by creating an issue. If you've discovered a bug or have a feature you'd like to see added to perceptrain, feel free to create an issue on perceptrain's GitHub issue tracker. Here are some steps to take:</p> <ol> <li>Quickly search the existing issues using relevant keywords to ensure your issue hasn't been addressed already.</li> <li> <p>If your issue is not listed, create a new one. Try to be as detailed and clear as possible in your description.</p> </li> <li> <p>If you're merely suggesting an improvement or reporting a bug, that's already excellent! We thank you for it. Your issue will be listed and, hopefully, addressed at some point.</p> </li> <li>However, if you're willing to be the one solving the issue, that would be even better! In such instances, you would proceed by preparing a Pull Request.</li> </ol>"},{"location":"getting_started/CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>We're excited that you're eager to contribute to perceptrain. To contribute, fork the <code>main</code> branch of perceptrain repository and once you are satisfied with your feature and all the tests pass create a Pull Request.</p> <p>Here's the process for making a contribution:</p> <p>Click the \"Fork\" button at the upper right corner of the repo page to create a new GitHub repo at <code>https://github.com/USERNAME/perceptrain</code>, where <code>USERNAME</code> is your GitHub ID. Then, <code>cd</code> into the directory where you want to place your new fork and clone it:</p> <pre><code>git clone https://github.com/USERNAME/perceptrain.git\n</code></pre> <p>Next, navigate to your new perceptrain fork directory and mark the main perceptrain repository as the <code>upstream</code>:</p> <pre><code>git remote add upstream https://github.com/pasqal-io/perceptrain.git\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>We recommended to use <code>hatch</code> for managing environments:</p> <p>To develop within perceptrain, use: <pre><code>pip install hatch\nhatch -v shell\n</code></pre></p> <p>To run perceptrain tests, use:</p> <pre><code>hatch -e tests run test\n</code></pre> <p>If you don't want to use <code>hatch</code>, you can use the environment manager of your choice (e.g. Conda) and execute the following:</p> <pre><code>pip install pytest\npip install -e .\npytest\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#useful-things-for-your-workflow-linting-and-testing","title":"Useful things for your workflow: linting and testing","text":"<p>Use <code>pre-commit</code> to lint your code and run the unit tests before pushing a new commit.</p> <p>Using <code>hatch</code>, it's simply:</p> <pre><code>hatch -e tests run pre-commit run --all-files\nhatch -e tests run test\n</code></pre> <p>Our CI/CD pipeline will also test if the documentation can be built correctly. To test it locally, please run:</p> <pre><code>hatch -e docs run mkdocs build --clean --strict\n</code></pre> <p>Without <code>hatch</code>, <code>pip</code> install those libraries first: \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"mkdocs-section-index\", \"mkdocs-jupyter\", \"mkdocs-exclude\", \"markdown-exec\"</p> <p>And then:</p> <pre><code> mkdocs build --clean --strict\n</code></pre>"},{"location":"getting_started/LICENSE/","title":"Apache License","text":"<p>Version 2.0, January 2004</p> <p>http://www.apache.org/licenses/</p>"},{"location":"getting_started/LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"getting_started/LICENSE/#1-definitions","title":"1. Definitions:","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"getting_started/LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"getting_started/LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"getting_started/LICENSE/#4-redistribution","title":"4. Redistribution.","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ul> <li> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> </li> <li> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> </li> <li> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> </li> <li> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> </li> </ul> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"getting_started/LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions.","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"getting_started/LICENSE/#6-trademarks","title":"6. Trademarks.","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"getting_started/LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty.","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"getting_started/LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability.","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"getting_started/LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability.","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"getting_started/LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":""},{"location":"getting_started/LICENSE/#appendix-how-to-apply-the-apache-license-to-your-work","title":"APPENDIX: How to apply the Apache License to your work.","text":"<p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!)  The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright [yyyy] Pasqal</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>Perceptrain is fully tested on Linux/MacOS operating systems. For Windows users, we recommend using WSL2 to install a Linux distribution of choice.</p>"},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>Perceptrain can be installed from PyPI with <code>pip</code> as follows:</p> <pre><code>pip install perceptrain\n</code></pre> <p>By default, this will also install PyQTorch, a differentiable state vector simulator which serves as the main numerical backend for Perceptrain.</p>"},{"location":"getting_started/installation/#install-from-source","title":"Install from source","text":"<p>We recommend to use the <code>hatch</code> environment manager to install <code>perceptrain</code> from source:</p> <pre><code>python -m pip install hatch\n\n# get into a shell with all the dependencies\npython -m hatch shell\n\n# run a command within the virtual environment with all the dependencies\npython -m hatch run python my_script.py\n</code></pre> <p>Warning</p> <p><code>hatch</code> will not combine nicely with other environment managers such Conda. If you want to use Conda, install it from source using <code>pip</code>:</p> <pre><code># within the Conda environment\npython -m pip install -e .\n</code></pre>"},{"location":"getting_started/installation/#citation","title":"Citation","text":"<p>If you use perceptrain for a publication, we kindly ask you to cite our work using the following BibTex entry:</p> <pre><code>@article{perceptrain2024pasqal,\n  title = {perceptrain},\n  author={Manu Lahariya},\n  year = {2025}\n}\n</code></pre>"},{"location":"tutorials/","title":"Introduction to perceptrain","text":"<p>Welcome to the <code>perceptrain</code> documentation. This library is designed to streamline your machine learning workflows \u2014especially for quantum machine learning\u2014 by providing a set of robust tools for training, monitoring, and optimizing your models.</p>"},{"location":"tutorials/#what-this-documentation-is-about","title":"What this documentation is about","text":"<ul> <li> <p>Trainer Class   Learn how to leverage the versatile <code>Trainer</code> class to manage your training loops, handle data loading, and integrate with experiment tracking tools like TensorBoard and MLflow. Detailed guides cover:</p> <ul> <li>Setting up training on both GPUs and CPUs.</li> <li>Configuring single-process, multi-processing, and distributed training setups.</li> </ul> </li> <li> <p>Gradient Optimization Methods   Explore both gradient-based and gradient-free optimization strategies. Find examples demonstrating how to switch between these modes and how to use context managers for mixed optimization.</p> </li> <li> <p>Custom Loss Functions and Hooks   Discover how to define custom loss functions tailored to your tasks and use hooks to insert custom behaviors at various stages of the training process.</p> </li> <li> <p>Callbacks for Enhanced Training   Utilize built-in and custom callbacks to log metrics, save checkpoints, adjust learning rates, and more. This section explains how to integrate callbacks seamlessly into your training workflow.</p> </li> <li> <p>Experiment Tracking   Understand how to configure experiment tracking with tools such as TensorBoard and MLflow to monitor your model\u2019s progress and performance.</p> </li> </ul>"},{"location":"tutorials/#getting-started","title":"Getting Started","text":"<p>To dive in, explore the detailed sections below:</p> <ul> <li>perceptrain Trainer Guide</li> <li>Training Configuration</li> <li>Callbacks for Trainer</li> <li>Accelerator for Distributed Training</li> <li>Training on GPU with Trainer</li> <li>Training on CPU with Trainer</li> </ul>"},{"location":"tutorials/callbacks/custom/","title":"Custom Callbacks","text":"<p>The base <code>Callback</code> class in perceptrain allows defining custom behavior that can be triggered at specified events (e.g., start of training, end of epoch). You can set parameters such as when the callback runs (<code>on</code>), frequency of execution (<code>called_every</code>), and optionally define a <code>callback_condition</code>.</p>"},{"location":"tutorials/callbacks/custom/#defining-callbacks","title":"Defining Callbacks","text":"<p>There are two main ways to define a callback: 1. Directly providing a function in the <code>Callback</code> instance. 2. Subclassing the <code>Callback</code> class and implementing custom logic.</p>"},{"location":"tutorials/callbacks/custom/#example-1-providing-a-callback-function-directly","title":"Example 1: Providing a Callback Function Directly","text":"<pre><code>from perceptrain.callbacks import Callback\n\n# Define a custom callback function\ndef custom_callback_function(trainer, config, writer):\n    print(\"Executing custom callback.\")\n\n# Create the callback instance\ncustom_callback = Callback(\n    on=\"train_end\",\n    callback=custom_callback_function\n)\n</code></pre>"},{"location":"tutorials/callbacks/custom/#example-2-subclassing-the-callback","title":"Example 2: Subclassing the Callback","text":"<pre><code>from perceptrain.callbacks import Callback\n\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in run_callback method.\")\n\n# Create the subclassed callback instance\ncustom_callback = CustomCallback(on=\"train_batch_end\", called_every=10)\n</code></pre>"},{"location":"tutorials/callbacks/default/","title":"Built-in Callbacks","text":"<p><code>perceptrain</code> offers several built-in callbacks for common tasks like saving checkpoints, logging metrics, and tracking models. Below is an overview of each.</p>"},{"location":"tutorials/callbacks/default/#1-printmetrics","title":"1. <code>PrintMetrics</code>","text":"<p>Prints metrics at specified intervals.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import PrintMetrics\n\nprint_metrics_callback = PrintMetrics(on=\"val_batch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[print_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#2-writemetrics","title":"2. <code>WriteMetrics</code>","text":"<p>Writes metrics to a specified logging destination.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import WriteMetrics\n\nwrite_metrics_callback = WriteMetrics(on=\"train_epoch_end\", called_every=50)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[write_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#3-plotmetrics","title":"3. <code>PlotMetrics</code>","text":"<p>Plots metrics based on user-defined plotting functions.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import PlotMetrics\n\nplot_metrics_callback = PlotMetrics(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[plot_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#4-loghyperparameters","title":"4. <code>LogHyperparameters</code>","text":"<p>Logs hyperparameters to keep track of training settings.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LogHyperparameters\n\nlog_hyper_callback = LogHyperparameters(on=\"train_start\", called_every=1)\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_hyper_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#5-savecheckpoint","title":"5. <code>SaveCheckpoint</code>","text":"<p>Saves model checkpoints at specified intervals.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import SaveCheckpoint\n\nsave_checkpoint_callback = SaveCheckpoint(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#6-savebestcheckpoint","title":"6. <code>SaveBestCheckpoint</code>","text":"<p>Saves the best model checkpoint based on a validation criterion.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import SaveBestCheckpoint\n\nsave_best_checkpoint_callback = SaveBestCheckpoint(on=\"val_epoch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_best_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#7-loadcheckpoint","title":"7. <code>LoadCheckpoint</code>","text":"<p>Loads a saved model checkpoint at the start of training.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LoadCheckpoint\n\nload_checkpoint_callback = LoadCheckpoint(on=\"train_start\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[load_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#8-logmodeltracker","title":"8. <code>LogModelTracker</code>","text":"<p>Logs the model structure and parameters.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LogModelTracker\n\nlog_model_callback = LogModelTracker(on=\"train_end\")\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_model_callback]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#9-lrschedulerstepdecay","title":"9. <code>LRSchedulerStepDecay</code>","text":"<p>Reduces the learning rate by a factor at regular intervals.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LRSchedulerStepDecay\n\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\", called_every=100, gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_step_decay]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#10-lrschedulercyclic","title":"10. <code>LRSchedulerCyclic</code>","text":"<p>Applies a cyclic learning rate schedule during training.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LRSchedulerCyclic\n\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\", called_every=1, base_lr=0.001, max_lr=0.01, step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cyclic]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#11-lrschedulercosineannealing","title":"11. <code>LRSchedulerCosineAnnealing</code>","text":"<p>Applies cosine annealing to the learning rate during training.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import LRSchedulerCosineAnnealing\n\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\", called_every=1, t_max=5000, min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cosine]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#12-earlystopping","title":"12. <code>EarlyStopping</code>","text":"<p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(on=\"val_epoch_end\", called_every=1, monitor=\"val_loss\", patience=5, mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[early_stopping]\n)\n</code></pre>"},{"location":"tutorials/callbacks/default/#13-gradientmonitoring","title":"13. <code>GradientMonitoring</code>","text":"<p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import GradientMonitoring\n\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre>"},{"location":"tutorials/callbacks/intro/","title":"Callbacks for Trainer","text":"<p>Perceptrain provides a powerful callback system for customizing various stages of the training process. With callbacks, you can monitor, log, save, and alter your training workflow efficiently. A <code>CallbackManager</code> is used with <code>Trainer</code> to execute the training process with defined callbacks. Following default callbacks are already provided in the <code>Trainer</code>.</p>"},{"location":"tutorials/callbacks/intro/#default-callbacks","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks already implemented in the <code>CallbackManager</code> used with <code>Trainer</code>:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>This guide covers how to define and use callbacks in <code>TrainConfig</code>, integrate them with the <code>Trainer</code> class, and create custom callbacks using hooks.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/callbacks/trainer_callbacks/","title":"Callbacks with Trainer","text":""},{"location":"tutorials/callbacks/trainer_callbacks/#1-adding-callbacks-to-trainconfig","title":"1. Adding Callbacks to <code>TrainConfig</code>","text":"<p>To use callbacks in <code>TrainConfig</code>, add them to the <code>callbacks</code> list when configuring the training process.</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.callbacks import SaveCheckpoint, PrintMetrics\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[\n        SaveCheckpoint(on=\"val_epoch_end\", called_every=50),\n        PrintMetrics(on=\"train_epoch_end\", called_every=100),\n    ]\n)\n</code></pre>"},{"location":"tutorials/callbacks/trainer_callbacks/#2-using-callbacks-with-trainer","title":"2. Using Callbacks with <code>Trainer</code>","text":"<p>The <code>Trainer</code> class in <code>perceptrain</code> provides built-in support for executing callbacks at various stages in the training process, managed through a callback manager. By default, several callbacks are added to specific hooks to automate common tasks, such as check-pointing, metric logging, and model tracking.</p>"},{"location":"tutorials/callbacks/trainer_callbacks/#default-callbacks","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks and their assigned hooks:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>These defaults handle common needs, but you can also add custom callbacks to any hook.</p>"},{"location":"tutorials/callbacks/trainer_callbacks/#example-adding-a-custom-callback","title":"Example: Adding a Custom Callback","text":"<p>To create a custom <code>Trainer</code> that includes a <code>PrintMetrics</code> callback executed specifically at the end of each epoch, follow the steps below.</p> <pre><code>from perceptrain.trainer import Trainer\nfrom perceptrain.callbacks import PrintMetrics\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.print_metrics_callback = PrintMetrics(on=\"train_epoch_end\", called_every = 10)\n\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        self.print_metrics_callback.run_callback(self)\n</code></pre>"},{"location":"tutorials/dataconfig/config/","title":"Training Configuration","text":"<p>The <code>TrainConfig</code> class provides a comprehensive configuration setup for training quantam machine learning models in perceptrain. This configuration includes settings for batch size, logging, check-pointing, validation, and additional custom callbacks that control the training process's granularity and flexibility.</p> <p>The <code>TrainConfig</code> tells <code>Trainer</code>  what batch_size should be used, how many epochs to train, in which intervals to print/log metrics and how often to store intermediate checkpoints. It is also possible to provide custom callback functions by instantiating a <code>Callback</code> with a function <code>callback</code>.</p> <p>For example of how to use the TrainConfig with <code>Trainer</code>, please see Examples in Trainer</p>"},{"location":"tutorials/dataconfig/config/#1-explanation-of-trainconfig-attributes","title":"1 Explanation of <code>TrainConfig</code> Attributes","text":"Attribute Type Default Description <code>max_iter</code> <code>int</code> <code>10000</code> Total number of training epochs. <code>batch_size</code> <code>int</code> <code>1</code> Batch size for training. <code>print_every</code> <code>int</code> <code>0</code> Frequency of console output. Set to <code>0</code> to disable. <code>write_every</code> <code>int</code> <code>0</code> Frequency of logging metrics. Set to <code>0</code> to disable. <code>plot_every</code> <code>int</code> <code>0</code> Frequency of plotting metrics. Set to <code>0</code> to disable. <code>checkpoint_every</code> <code>int</code> <code>0</code> Frequency of saving checkpoints. Set to <code>0</code> to disable. <code>val_every</code> <code>int</code> <code>0</code> Frequency of validation checks. Set to <code>0</code> to disable. <code>val_epsilon</code> <code>float</code> <code>1e-5</code> Threshold for validation improvement. <code>validation_criterion</code> <code>Callable</code> <code>None</code> Function for validating metric improvement. <code>trainstop_criterion</code> <code>Callable</code> <code>None</code> Function to stop training early. <code>callbacks</code> <code>list[Callback]</code> <code>[]</code> List of custom callbacks. <code>root_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Root directory for saving logs and checkpoints. <code>log_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Logging directory for saving logs and checkpoints. <code>log_model</code> <code>bool</code> <code>False</code> Enables model logging. <code>verbose</code> <code>bool</code> <code>True</code> Enables detailed logging. <code>tracking_tool</code> <code>ExperimentTrackingTool</code> <code>TENSORBOARD</code> Tool for tracking training metrics. <code>plotting_functions</code> <code>tuple</code> <code>()</code> Functions for plotting metrics. <code>hyperparams</code> <code>dict</code> <code>{}</code> Dictionary of hyperparameters <code>nprocs</code> <code>int</code> <code>1</code> Number of processes to use when spawning subprocesses; for multi-GPU setups, set this to the total number of GPUs. <code>compute_setup</code> <code>str</code> <code>\"cpu\"</code> Specifies the compute device: <code>\"auto\"</code>, <code>\"gpu\"</code>, or <code>\"cpu\"</code>. <code>backend</code> <code>str</code> <code>\"gloo\"</code> Backend for distributed training communication (e.g., <code>\"gloo\"</code>, <code>\"nccl\"</code>, or <code>\"mpi\"</code>). <code>log_setup</code> <code>str</code> <code>\"cpu\"</code> Device setup for logging; use <code>\"cpu\"</code> to avoid GPU conflicts <code>dtype</code> <code>dtype</code> or <code>None</code> <code>None</code> Data type for computations (e.g., <code>torch.float32</code>) <code>all_reduce_metrics</code> <code>bool</code> <code>False</code> If <code>True</code>, aggregates metrics (e.g., loss) across processes <pre><code>from perceptrain import OptimizeResult, TrainConfig\nfrom perceptrain.callbacks import Callback\n\nbatch_size = 5\nn_epochs = 100\n\nprint_parameters = lambda opt_res: print(opt_res.model.parameters())\ncondition_print = lambda opt_res: opt_res.loss &lt; 1.0e-03\nmodify_extra_opt_res = {\"n_epochs\": n_epochs}\ncustom_callback = Callback(on=\"train_end\", callback = print_parameters, callback_condition=condition_print, modify_optimize_result=modify_extra_opt_res, called_every=10,)\n\nconfig = TrainConfig(\n    root_folder=\"some_path/\",\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n    callbacks = [custom_callback]\n)\n</code></pre>"},{"location":"tutorials/dataconfig/config/#2-key-configuration-options-in-trainconfig","title":"2 Key Configuration Options in <code>TrainConfig</code>","text":""},{"location":"tutorials/dataconfig/config/#iterations-and-batch-size","title":"Iterations and Batch Size","text":"<ul> <li><code>max_iter</code> (int): Specifies the total number of training iterations (epochs). For an <code>InfiniteTensorDataset</code>, each epoch contains one batch; for a <code>TensorDataset</code>, it contains <code>len(dataloader)</code> batches.</li> <li><code>batch_size</code> (int): Defines the number of samples processed in each training iteration.</li> </ul> <p>Example: <pre><code>config = TrainConfig(max_iter=2000, batch_size=32)\n</code></pre></p>"},{"location":"tutorials/dataconfig/config/#training-parameters","title":"Training Parameters","text":"<ul> <li><code>print_every</code> (int): Controls how often loss and metrics are printed to the console.</li> <li><code>write_every</code> (int): Determines how frequently metrics are written to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>checkpoint_every</code> (int): Sets the frequency for saving model checkpoints.</li> </ul> <p>Note: Set 0 to diable.</p> <p>Example: <pre><code>config = TrainConfig(print_every=100, write_every=50, checkpoint_every=50)\n</code></pre></p> <p>The user can provide either the <code>root_folder</code> or the <code>log_folder</code> for saving checkpoints and logging. When neither are provided, the default <code>root_folder</code> \"./qml_logs\" is used.</p> <ul> <li><code>root_folder</code> (Path): The root directory for saving checkpoints and logs. All training logs will be saved inside a subfolder in this root directory. (The path to these subfolders can be accessed using config._subfolders, and the current logging folder is config.log_folder)</li> <li><code>create_subfolder_per_run</code> (bool): Creates a unique subfolder for each training run within the specified folder.</li> <li><code>tracking_tool</code> (ExperimentTrackingTool): Specifies the tracking tool to log metrics, e.g., TensorBoard or MLflow.</li> <li><code>log_model</code> (bool): Enables logging of a serialized version of the model, which is useful for model versioning. Thi happens at the end of training.</li> </ul> <p>Note     - The user can also provide <code>log_folder</code> argument - which will only be used when <code>create_subfolder_per_run</code> = False.     -  <code>log_folder</code> (Path): The log folder used for saving checkpoints and logs.</p> <p>Example: <pre><code>config = TrainConfig(root_folder=\"path/to/checkpoints\", tracking_tool=ExperimentTrackingTool.MLFLOW, checkpoint_best_only=True)\n</code></pre></p>"},{"location":"tutorials/dataconfig/config/#validation-parameters","title":"Validation Parameters","text":"<ul> <li><code>checkpoint_best_only</code> (bool): If set to <code>True</code>, saves checkpoints only when there is an improvement in the validation metric.</li> <li><code>val_every</code> (int): Frequency of validation checks. Setting this to <code>0</code> disables validation.</li> <li><code>val_epsilon</code> (float): A small threshold used to compare the current validation loss with previous best losses.</li> <li><code>validation_criterion</code> (Callable): A custom function to assess if the validation metric meets a specified condition.</li> </ul> <p>Example: <pre><code>config = TrainConfig(val_every=200, checkpoint_best_only = True, validation_criterion=lambda current, best: current &lt; best - 0.001)\n</code></pre></p> <p>If it is desired to only the save the \"best\" checkpoint, the following must be ensured:</p> <pre><code>(a) `checkpoint_best_only = True` is used while creating the configuration through `TrainConfig`,\n(b) `val_every` is set to a valid integer value (for example, `val_every = 10`) which controls the no. of iterations after which the validation data should be used to evaluate the model during training, which can also be set through `TrainConfig`,\n(c) a validation criterion is provided through the `validation_criterion`, set through `TrainConfig` to quantify the definition of \"best\", and\n(d) the validation dataloader passed to `Trainer` is of type `DataLoader`. In this case, it is expected that a validation dataloader is also provided along with the train dataloader since the validation data will be used to decide the \"best\" checkpoint.\n</code></pre> <p>The criterion used to decide the \"best\" checkpoint can be customized by <code>validation_criterion</code>, which should be a function that can take val_loss, best_loss, and val_epsilon arguments and return a boolean value (True or False) indicating whether some validation metric is satisfied or not. An example of a simple <code>validation_criterion</code> is: <pre><code>def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:\n    return val_loss &lt; (best_val_loss - val_epsilon)\n</code></pre></p>"},{"location":"tutorials/dataconfig/config/#custom-callbacks","title":"Custom Callbacks","text":"<p><code>TrainConfig</code> supports custom callbacks that can be triggered at specific stages of training. The <code>callbacks</code> attribute accepts a list of callback instances, which allow for custom behaviors like early stopping or additional logging. See Callbacks for more details.</p> <ul> <li><code>callbacks</code> (list[Callback]): List of custom callbacks to execute during training.</li> </ul> <p>Example: <pre><code>from perceptrain.callbacks import Callback\n\ndef callback_fn(trainer, config, writer):\n    if trainer.opt_res.loss &lt; 0.001:\n        print(\"Custom Callback: Loss threshold reached!\")\n\ncustom_callback = Callback(on = \"train_epoch_end\", called_every = 10, callback_function = callback_fn )\n\nconfig = TrainConfig(callbacks=[custom_callback])\n</code></pre></p>"},{"location":"tutorials/dataconfig/config/#hyperparameters-and-plotting","title":"Hyperparameters and Plotting","text":"<ul> <li><code>hyperparams</code> (dict): A dictionary of hyperparameters (e.g., learning rate, regularization) to be tracked by the tracking tool.</li> <li><code>plot_every</code> (int): Determines how frequently plots are saved to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>plotting_functions</code> (tuple[LoggablePlotFunction, ...]): Functions for in-training plotting of metrics or model state.</li> </ul> <p>Note: Please ensure that plotting_functions are provided when plot_every &gt; 0</p> <p>Example: <pre><code>config = TrainConfig(\n    plot_every=10,\n    hyperparams={\"learning_rate\": 0.001, \"batch_size\": 32},\n    plotting_functions=(plot_loss_function,)\n)\n</code></pre></p>"},{"location":"tutorials/dataconfig/config/#advanced-distributed-training","title":"Advanced Distributed Training","text":"<ul> <li> <p><code>nprocs</code> (int): Specifies the number of processes to be used. For multi-GPU training, this should match the total number of GPUs available. When nprocs is greater than 1, <code>Trainer</code> spawns additional subprocesses for training. This is useful for parallel or distributed training setups.</p> </li> <li> <p><code>compute_setup</code> (str): Determines the compute device configuration: 1.<code>\"auto\"</code> (automatically selects GPU if available),  <code>\"gpu\"</code> - (forces GPU usage and errors if no GPU is detected), and 3. <code>\"cpu\"</code> (Forces the use of the CPU).</p> </li> <li> <p><code>backend</code> (str): Specifies the communication backend for distributed training. Common options are <code>\"gloo\"</code> (default), <code>\"nccl\"</code> (optimized for GPUs), or <code>\"mpi\"</code>, depending on your setup. It should be one of the backends supported by <code>torch.distributed</code>. For further details, please look at torch backends</p> </li> </ul> <p>Notes: - Logging Specific Callbacks: Logging is available only through the main process, i.e. process 0.  Model logging, plotting, logging metrics will only be performed for a single process, even if multiple processes are run. - Training with specific callbacks: Callbacks specific to training, e.g., <code>EarlyStopping</code>, <code>LRSchedulerStepDecay</code>, etc will be called from each process. - <code>PrintMetrics</code> (set through the <code>print_every</code> argument in <code>TrainCongig</code>) is available from all processes.</p> <p>Example: For CPU MultiProcessing <pre><code>config = TrainConfig(\n    compute_setup=\"cpu\",\n    nprocs=5,\n    backend=\"gloo\"\n)\n</code></pre></p> <p>Example: For GPU multiprocessing training <pre><code>config = TrainConfig(\n    compute_setup=\"gpu\",\n    nprocs=2, # World-size/Total number of GPUs\n    backend=\"nccl\"\n)\n</code></pre></p>"},{"location":"tutorials/dataconfig/config/#precision-options","title":"Precision Options","text":"<ul> <li> <p><code>dtype</code> (dtype or None): Sets the numerical precision (data type) for computations. For instance, you can use <code>torch.float32</code> or <code>torch.float16</code> depending on your performance and precision needs. Both model parameters, and dataset will be of the provided precision.</p> <ul> <li>If not specified or None, the default torch precision (usually torch.float32) is used.</li> <li>If provided dtype is complex dtype, appropriate precision for the data and model parameters will be used as follows:</li> </ul> Data Type (<code>dtype</code>) Data Precision Model Precision Model Parameters Precision  (Real Part  &amp; Imaginary Part ) <code>torch.float16</code> 16-bit 16-bit N/A <code>torch.float32</code> 32-bit 32-bit N/A <code>torch.float64</code> 64-bit 64-bit N/A <code>torch.complex32</code> 16-bit 32-bit 16-bit <code>torch.complex64</code> 32-bit 64-bit 32-bit <code>torch.complex128</code> 64-bit 128-bit 64-bit <p>Complex Dtypes: Complex data types are useful for Quantum Neural Networks - such as <code>QNN</code> provided by perceptrain. The industry standard is to use <code>torch.complex128</code>, however, the user can also specify a lower precision (<code>torch.complex64</code> or  <code>torch.complex32</code>) for faster training.</p> </li> </ul> <p>Furthermore, the user can also utilize the following options:</p> <ul> <li> <p><code>log_setup</code> (str): Configures the device used for logging. Using <code>\"cpu\"</code> ensures logging runs on the CPU (which may avoid conflicts with GPU operations), while <code>\"auto\"</code> aligns logging with the compute device.</p> </li> <li> <p><code>all_reduce_metrics</code> (bool): When enabled, aggregates metrics (such as loss or accuracy) across all training processes to provide a unified summary, though it may introduce additional synchronization overhead.</p> </li> </ul>"},{"location":"tutorials/dataconfig/data/","title":"Dataloaders","text":"<p>When using Perceptrain, you can supply classical data to a quantum machine learning algorithm by using a standard PyTorch <code>DataLoader</code> instance. Perceptrain also provides the <code>DictDataLoader</code> convenience class which allows to build dictionaries of <code>DataLoader</code>s instances and easily iterate over them.</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom perceptrain import DictDataLoader, to_dataloader\n\n\ndef dataloader(data_size: int = 25, batch_size: int = 5, infinite: bool = False) -&gt; DataLoader:\n    x = torch.linspace(0, 1, data_size).reshape(-1, 1)\n    y = torch.sin(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=infinite)\n\n\ndef dictdataloader(data_size: int = 25, batch_size: int = 5) -&gt; DictDataLoader:\n    dls = {}\n    for k in [\"y1\", \"y2\"]:\n        x = torch.rand(data_size, 1)\n        y = torch.sin(x)\n        dls[k] = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n    return DictDataLoader(dls)\n\n\n# iterate over standard DataLoader\nfor (x,y) in dataloader(data_size=6, batch_size=2):\n    print(f\"Standard {x = }\")\n\n# construct an infinite dataset which will keep sampling indefinitely\nn_epochs = 5\ndl = iter(dataloader(data_size=6, batch_size=2, infinite=True))\nfor _ in range(n_epochs):\n    (x, y) = next(dl)\n    print(f\"Infinite {x = }\")\n\n# iterate over DictDataLoader\nddl = dictdataloader()\ndata = next(iter(ddl))\nprint(f\"{data = }\")\n</code></pre> <pre><code>Standard x = tensor([[0.0000],\n        [0.2000]])\nStandard x = tensor([[0.4000],\n        [0.6000]])\nStandard x = tensor([[0.8000],\n        [1.0000]])\nInfinite x = tensor([[0.0000],\n        [0.8000]])\nInfinite x = tensor([[0.4000],\n        [0.6000]])\nInfinite x = tensor([[1.0000],\n        [0.2000]])\nInfinite x = tensor([[0.0000],\n        [0.8000]])\nInfinite x = tensor([[0.4000],\n        [0.6000]])\ndata = {'y1': [tensor([[0.5372],\n        [0.4559],\n        [0.4736],\n        [0.2233],\n        [0.7577]]), tensor([[0.5117],\n        [0.4403],\n        [0.4561],\n        [0.2215],\n        [0.6873]])], 'y2': [tensor([[0.6617],\n        [0.5407],\n        [0.4321],\n        [0.0251],\n        [0.5718]]), tensor([[0.6145],\n        [0.5148],\n        [0.4188],\n        [0.0251],\n        [0.5411]])]}\n</code></pre> <p>Note:     In case of <code>infinite</code>=True, the dataloader iterator will provide a random sample from the dataset.</p>"},{"location":"tutorials/dataconfig/intro/","title":"Configurations for Trainer","text":"<p>Perceptrain provides a <code>TrainConfig</code> to define the parameters for the Trainer, along with various <code>Dataloaders</code> designed to work with machine learning, physics informed machine learning, and quantum machine learning.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/dataconfig/tracking/","title":"Experiment tracking with mlflow","text":"<p>perceptrain allows to track runs and log hyperparameters, models and plots with tensorboard and mlflow. In the following, we demonstrate the integration with mlflow.</p>"},{"location":"tutorials/dataconfig/tracking/#mlflow-configuration","title":"mlflow configuration","text":"<p>We have control over our tracking configuration by setting environment variables. First, let's look at the tracking URI. For the purpose of this demo we will be working with a local database, in a similar fashion as described here, <pre><code>export MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n</code></pre></p> <p>perceptrain can also read the following two environment variables to define the mlflow experiment name and run name <pre><code>export MLFLOW_EXPERIMENT=test_experiment\nexport MLFLOW_RUN_NAME=run_0\n</code></pre></p> <p>If no tracking URI is provided, mlflow stores run information and artifacts in the local <code>./mlflow</code> directory and if no names are defined, the experiment and run will be named with random UUIDs.</p>"},{"location":"tutorials/distributed/CPU/","title":"Training on CPU with <code>Trainer</code>","text":"<p>This guide explains how to train models on CPU using <code>Trainer</code> from <code>perceptrain</code>, covering single-process and multi-processing setups.</p>"},{"location":"tutorials/distributed/CPU/#understanding-arguments","title":"Understanding Arguments","text":"<ul> <li>nprocs: Number of processes to run. To enable multi-processing and launch separate processes, set nprocs &gt; 1.</li> <li>compute_setup: The computational setup used for training. Options include <code>cpu</code>, <code>gpu</code>, and <code>auto</code>.</li> </ul> <p>For more details on the advanced training options, please refer to TrainConfig Documentation</p>"},{"location":"tutorials/distributed/CPU/#configuring-trainconfig-for-cpu-training","title":"Configuring <code>TrainConfig</code> for CPU Training","text":"<p>By adjusting <code>TrainConfig</code>, you can seamlessly switch between single and multi-core CPU training. To enable CPU-based training, update these fields in <code>TrainConfig</code>:</p>"},{"location":"tutorials/distributed/CPU/#single-process-training-configuration","title":"Single-Process Training Configuration:","text":"<ul> <li><code>backend=\"cpu\"</code>: Ensures training runs on the CPU.</li> <li><code>nprocs=1</code>: Uses one CPU core.</li> </ul> <pre><code>train_config = TrainConfig(\n    compute_setup=\"cpu\",\n)\n</code></pre>"},{"location":"tutorials/distributed/CPU/#multi-processing-configuration","title":"Multi-Processing Configuration","text":"<ul> <li><code>backend=\"gloo\"</code>: Uses the Gloo backend for CPU multi-processing.</li> <li><code>nprocs=4</code>: Utilizes 4 CPU cores.</li> </ul> <pre><code>train_config = TrainConfig(\n    compute_setup=\"cpu\",\n    backend=\"gloo\",\n    nprocs=4,\n)\n</code></pre>"},{"location":"tutorials/distributed/GPU/","title":"Training on GPU with <code>Trainer</code>","text":"<p>This guide explains how to train models on GPU using <code>Trainer</code> from <code>perceptrain</code>, covering single-GPU, multi-GPU (single node), and multi-node multi-GPU setups.</p>"},{"location":"tutorials/distributed/GPU/#understanding-arguments","title":"Understanding Arguments","text":"<ul> <li>nprocs: Number of processes to run. To enable multi-processing and launch separate processes, set nprocs &gt; 1.</li> <li>compute_setup: The computational setup used for training. Options include <code>cpu</code>, <code>gpu</code>, and <code>auto</code>.</li> </ul> <p>For more details on the advanced training options, please refer to TrainConfig Documentation</p>"},{"location":"tutorials/distributed/GPU/#configuring-trainconfig-for-gpu-training","title":"Configuring <code>TrainConfig</code> for GPU Training","text":"<p>By adjusting <code>TrainConfig</code>, you can switch between single and multi-GPU training setups. Below are the key settings for each configuration:</p>"},{"location":"tutorials/distributed/GPU/#single-gpu-training-configuration","title":"Single-GPU Training Configuration:","text":"<ul> <li><code>compute_setup</code>: Selected training setup. (<code>gpu</code> or <code>auto</code>)</li> <li><code>backend=\"nccl\"</code>: Optimized backend for GPU training.</li> <li><code>nprocs=1</code>: Uses one GPU. <pre><code>train_config = TrainConfig(\n    compute_setup=\"auto\",\n    backend=\"nccl\",\n    nprocs=1,\n)\n</code></pre></li> </ul>"},{"location":"tutorials/distributed/GPU/#multi-gpu-single-node-training-configuration","title":"Multi-GPU (Single Node) Training Configuration:","text":"<ul> <li><code>compute_setup</code>: Selected training setup. (<code>gpu</code> or <code>auto</code>)</li> <li><code>backend=\"nccl\"</code>: Multi-GPU optimized backend.</li> <li><code>nprocs=2</code>: Utilizes 2 GPUs on a single node. <pre><code>train_config = TrainConfig(\n    compute_setup=\"auto\",\n    backend=\"nccl\",\n    nprocs=2,\n)\n</code></pre></li> </ul>"},{"location":"tutorials/distributed/GPU/#multi-node-multi-gpu-training-configuration","title":"Multi-Node Multi-GPU Training Configuration:","text":"<ul> <li><code>compute_setup</code>: Selected training setup. (<code>gpu</code> or <code>auto</code>)</li> <li><code>backend=\"nccl\"</code>: Required for multi-node setups.</li> <li><code>nprocs=4</code>: Uses 4 GPUs across nodes. <pre><code>train_config = TrainConfig(\n    compute_setup=\"auto\",\n    backend=\"nccl\",\n    nprocs=4,\n)\n</code></pre></li> </ul>"},{"location":"tutorials/distributed/GPU/#examples","title":"Examples","text":"<p>The next sections provide Python scripts and training approach scripts for each setup.</p> <p>Some organizations use SLURM to manage resources. Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. If you are using slurm, you can use the <code>Trainer</code> by submitting a batch script using sbatch.</p> <p>You can also use <code>torchrun</code> to run the training process - which provides a superset of the functionality as <code>torch.distributed.launch</code>. Here you need to specify the torchrun arguments arguments to set up the distributed training setup. We also include the <code>torchrun</code> sbatch scripts for each setup below.</p>"},{"location":"tutorials/distributed/accelerator_doc/","title":"Accelerator for Distributed Training","text":""},{"location":"tutorials/distributed/accelerator_doc/#overview","title":"Overview","text":"<p>The <code>Accelerator</code> class is designed to simplify distributed training with PyTorch's API. It allows for efficient training across multiple GPUs or processes while handling device placement, data distribution, and model synchronization. It uses <code>DistDataParallel</code> and <code>DistributedSampler</code> in the background to correctly distribute the model and training data across processes and devices.</p> <p>This tutorial will guide you through setting up and using <code>Accelerator</code> for distributed training.</p>"},{"location":"tutorials/distributed/accelerator_doc/#accelerator","title":"Accelerator","text":"<p>The <code>Accelerator</code> class manages the training environment and process distribution. Here\u2019s how you initialize it:</p> <pre><code>from perceptrain.train_utils import Accelerator\nimport torch\n\naccelerator = Accelerator(\n    nprocs=4,               # Number of processes (e.g., GPUs). Enables multiprocessing.\n    compute_setup=\"auto\",   # Automatically selects available compute devices\n    log_setup=\"cpu\",        # Logs on CPU to avoid memory overhead\n    dtype=torch.float32,    # Data type for numerical precision\n    backend=\"nccl\"          # Backend for communication\n)\n</code></pre>"},{"location":"tutorials/distributed/accelerator_doc/#accelerator-features","title":"Accelerator features","text":"<p>The <code>Accelerator</code> also provides a <code>distribute()</code> function wrapper that simplifies running distributed training across multiple processes. This method can be used to prepare or wrap a function that needs to be distributed.</p> <ul> <li> <p><code>distribute()</code></p> <p>This method allows you to wrap your training function so it runs across multiple processes, handling rank management and process spawning automatically.</p> <p>Example Usage: <pre><code>distributed_fun = accelerator.distribute(fun)\ndistributed_fun(*args, **kwargs)\n</code></pre></p> <p>The <code>distribute()</code> function ensures that each process runs on a designated device and synchronizes properly, making it easier to scale training with minimal code modifications.</p> <p>NOTE: <code>fun</code> should be Pickleable: Using <code>distribute</code> on <code>fun</code> allows user to spawn multiple processes that run <code>fun</code> using <code>torch.multiprocessing</code>. As a requirment for <code>torch.multiprocessing</code>,<code>fun</code> should be pickleable. It can either be a bounded class method, or an unabounded method defined in <code>__main__</code>.</p> </li> </ul> <p>The <code>Accelerator</code> further offers these key methods: <code>prepare</code>, <code>prepare_batch</code>, and <code>all_reduce_dict</code>.</p> <ul> <li> <p><code>prepare()</code></p> <p>This method ensures that models, optimizers, and dataloaders are properly placed on the correct devices for distributed training. It wraps models into <code>DistributedDataParallel</code> and synchronizes parameters across processes.</p> <pre><code>model, optimizer, dataloader = accelerator.prepare(model,\n                                                    optimizer,\n                                                    dataloader)\n</code></pre> </li> <li> <p><code>prepare_batch()</code>     Moves data batches to the correct device and formats them properly for distributed training.</p> <pre><code>batch_data, batch_targets = accelerator.prepare(batch)\n</code></pre> </li> <li> <p><code>all_reduce_dict()</code>     Aggregates and synchronizes metrics across all processes during training. Note: This will cause a synchronization overhead and slow down the training processes.</p> <pre><code>metrics = {\"loss\": torch.tensor(1.0)}\nreduced_metrics = accelerator.all_reduce_dict(metrics)\nprint(reduced_metrics)\n</code></pre> </li> </ul>"},{"location":"tutorials/distributed/accelerator_doc/#example","title":"Example","text":"<p>To launch distributed training across multiple GPUs/CPUs, use the following approach: Each batch should be moved to the correct device. The <code>prepare_batch()</code> method simplifies this process.</p>"},{"location":"tutorials/distributed/accelerator_doc/#example-code-train_scriptpy","title":"Example Code (train_script.py):","text":"<pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom perceptrain.train_utils import Accelerator\n\n\ndef train_epoch(epochs, model, dataloader, optimizer, accelerator):\n\n    # Prepare model, optimizer, and dataloader for distributed training\n    model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n\n    # accelerator.rank will provide you the rank of the process.\n    if accelerator.rank == 0:\n        print(\"Prepared model of type: \", type(model))\n        print(\"Prepared optimizer of type: \", type(optimizer))\n        print(\"Prepared dataloader of type: \", type(dataloader))\n\n    model.train()\n    for epoch in range(epochs):\n        for batch in dataloader:\n\n            # Move batch to the correct device\n            batch = accelerator.prepare_batch(batch)\n\n            batch_data, batch_targets = batch\n            optimizer.zero_grad()\n            output = model(batch_data)\n            loss = torch.nn.functional.mse_loss(output, batch_targets)\n            loss.backward()\n            optimizer.step()\n        print(\"Rank: \", accelerator.rank, \" | Epoch: \", epoch, \" | Loss: \", loss.item())\n\nif __name__ == \"__main__\":\n    n_epochs = 10\n    model = nn.Sequential(\n        nn.Linear(10, 100),  # Input Layer\n        nn.ReLU(),  # Activation Function\n        nn.Linear(100, 1)  # Output Layer\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    # A random dataset with 10 features and a target to predict.\n    dataset = TensorDataset(torch.randn(100, 10), torch.randn(100, 1))\n    dataloader = DataLoader(dataset, batch_size=32)\n\n    accelerator = Accelerator(\n        nprocs=4,               # Number of processes (e.g., GPUs). Enables multiprocessing.\n        compute_setup=\"cpu\",    # or choose GPU\n        backend=\"gloo\"          # choose `nccl` for GPU\n    )\n\n    distributed_train_epoch = accelerator.distribute(train_epoch)\n    distributed_train_epoch(n_epochs, model, dataloader, optimizer, accelerator)\n</code></pre>"},{"location":"tutorials/distributed/accelerator_doc/#running-distributed-training","title":"Running Distributed Training","text":"<p>The above example can be directly run on the terminal as following:</p> <pre><code>python train_script.py\n</code></pre> <ul> <li> <p>SLURM:</p> <p>To launch distributed training across multiple GPUs</p> <p>Inside an interactive <code>srun</code> session, you can directly use: <pre><code>python train_script.py\n</code></pre></p> <p>Or submit the following sbatch script: <pre><code>#!/bin/bash\n#SBATCH --job-name=MG_slurm\n#SBATCH --nodes=1\n#SBATCH --ntasks=1              # tasks = number of nodes\n#SBATCH --gpus-per-task=4       # same as nprocs\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=56G\n\nsrun python3 train_script.py\n</code></pre></p> </li> <li> <p>Torchrun:</p> <p>To run distributed training with <code>torchrun</code> <pre><code>#!/bin/bash\n#SBATCH --job-name=MG_torchrun\n#SBATCH --nodes=1\n#SBATCH --ntasks=1              # tasks = number of nodes\n#SBATCH --gpus-per-task=2       # same as nprocs\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=56G\n\nnodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )\nnodes_array=($nodes)\nhead_node=${nodes_array[0]}\nhead_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$head_node\" hostname -I | awk '{print $1}')\n\nsrun torchrun --nnodes 1 --nproc_per_node 2 --rdzv_id $RANDOM --rdzv_backend c10d --rdzv_endpoint $head_node_ip:29522 train_script.py\n</code></pre></p> </li> </ul>"},{"location":"tutorials/distributed/accelerator_doc/#conclusion","title":"Conclusion","text":"<p>The <code>Accelerator</code> class simplifies distributed training by handling process management, device setup, and data distribution. By integrating it into your PyTorch training workflow, you can efficiently scale training across multiple devices with minimal code modifications.</p>"},{"location":"tutorials/distributed/intro/","title":"Distributed Training","text":"<p><code>Trainer</code> from <code>perceptrain</code> supports distributed training across multiple CPUs and GPUs. This is achieved using the <code>Accelerator</code> provided by <code>perceptrain</code>, which can also be used to design custom distributed training loops.</p>"},{"location":"tutorials/distributed/intro/#configurations","title":"Configurations:","text":"<ul> <li><code>compute_setup</code>: Selected training setup. (<code>gpu</code> or <code>auto</code>)</li> <li><code>backend=\"nccl\"</code>: Optimized backend for GPU training.</li> <li><code>nprocs=1</code>: Uses one GPU. <pre><code>train_config = TrainConfig(\n    compute_setup=\"auto\",\n    backend=\"nccl\",\n    nprocs=1,\n)\n</code></pre></li> </ul>"},{"location":"tutorials/distributed/intro/#using-accelerator-with-trainer","title":"Using Accelerator with Trainer","text":"<p><code>Accelerator</code> is already integrated into the <code>Trainer</code> class from <code>perceptrain</code>, and <code>Trainer</code> can automatically distribute the training process based on the configurations provided in <code>TrainConfig</code>.</p> <pre><code>from perceptrain.trainer import Trainer\nfrom perceptrain import TrainConfig\n\nconfig = TrainConfig(nprocs=4)\n\ntrainer = Trainer(model, optimizer, config)\nmodel, optimizer = trainer.fit(dataloader)\n</code></pre> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/distributed/multi_cpu/","title":"Multi-Processing CPU Training Example","text":"<p>Multi-Processing Training: Best for large datasets, utilizes multiple CPU processes. Use <code>backend=\"gloo\"</code> and set <code>nprocs</code>.</p> <pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom perceptrain import TrainConfig, Trainer\nfrom perceptrain.optimize_step import optimize_step\nTrainer.set_use_grad(True)\n\n# __main__ is recommended.\nif __name__ == \"__main__\":\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    y = torch.sin(2 * torch.pi * x)\n    dataloader = DataLoader(TensorDataset(x, y), batch_size=16, shuffle=True)\n    model = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n    # Multi-Process Training Configuration\n    train_config = TrainConfig(\n        compute_setup=\"cpu\",\n        backend=\"gloo\",\n        nprocs=4,\n        max_iter=5,\n        print_every=1)\n\n    trainer = Trainer(model, optimizer, train_config, loss_fn=\"mse\", optimize_step=optimize_step)\n    trainer.fit(dataloader)\n</code></pre> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/distributed/multi_gpu/","title":"Multi GPU Training","text":"<p>The following section provide Python scripts and training approach scripts for Multi GPU Training.</p> <p>Some organizations use SLURM to manage resources. Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. If you are using slurm, you can use the <code>Trainer</code> by submitting a batch script using sbatch. Further below, we also provide the sbatch scripts for each setup.</p> <p>You can also use <code>torchrun</code> to run the training process - which provides a superset of the functionality as <code>torch.distributed.launch</code>. Here you need to specify the torchrun arguments arguments to set up the distributed training setup. We also include the <code>torchrun</code> sbatch scripts for each setup below.</p>"},{"location":"tutorials/distributed/multi_gpu/#example-training-script-trainpy","title":"Example Training Script (<code>train.py</code>):","text":"<p>We are going to use the following training script for the examples below. Python Script: <pre><code>import torch\nimport argparse\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom perceptrain import TrainConfig, Trainer\nfrom perceptrain.optimize_step import optimize_step\nTrainer.set_use_grad(True)\n\n# __main__ is recommended.\nif __name__ == \"__main__\":\n    # simple dataset for y = 2\u03c0x\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    y = torch.sin(2 * torch.pi * x)\n    dataloader = DataLoader(TensorDataset(x, y), batch_size=16, shuffle=True)\n    # Simple model with no hidden layer and ReLU activation to fit the data for y = 2\u03c0x\n    model = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n    # SGD optimizer with 0.01 learning rate\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n    # TrainConfig\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--nprocs\", type=int,\n                        default=1, help=\"Number of processes (GPUs) to use.\")\n    parser.add_argument(\"--compute_setup\", type=str,\n                        default=\"auto\", choices=[\"cpu\", \"gpu\", \"auto\"], help=\"Computational Setup.\")\n    parser.add_argument(\"--backend\", type=str,\n                        default=\"nccl\", choices=[\"nccl\", \"gloo\", \"mpi\"], help=\"Distributed backend.\")\n    args = parser.parse_args()\n    train_config = TrainConfig(\n                                backend=args.backend,\n                                nprocs=args.nprocs,\n                                compute_setup=args.compute_setup,\n                                print_every=5,\n                                max_iter=50\n                            )\n\n    trainer = Trainer(model, optimizer, train_config, loss_fn=\"mse\", optimize_step=optimize_step)\n    trainer.fit(dataloader)\n</code></pre></p>"},{"location":"tutorials/distributed/multi_gpu/#multi-gpu-single-node","title":"Multi-GPU (Single Node):","text":"<p>For high performance using multiple GPUs in one node. - Assuming that you have 1 node with 2 GPU. These numbers can be changed depending on user needs.</p> <p>You can train by simply calling this on the head node. <pre><code>python3 train.py --backend nccl --nprocs 2\n</code></pre></p>"},{"location":"tutorials/distributed/multi_gpu/#slurm","title":"SLURM","text":"<p>Slurm can be used to train the model but also to dispatch the workload on multiple GPUs or CPUs. - Here, we should have one task per gpu. i.e. <code>ntasks</code> is equal to the number of nodes - <code>nprocs</code> should be equal to the total number of gpus (world_size). which is this case is 2.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=multi_gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --gpus-per-task=2\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\nsrun python3 train.py --backend nccl --nprocs 2\n</code></pre>"},{"location":"tutorials/distributed/multi_gpu/#torchrun","title":"TORCHRUN","text":"<p>Torchrun takes care of setting the <code>nprocs</code> based on the cluster setup. We only need to specify to use the <code>compute_setup</code>, which can be either <code>auto</code> or <code>gpu</code>. - <code>nnodes</code> for torchrun should be the number of nodes - <code>nproc_per_node</code> should be equal to the number of GPUs per node.</p> <p>Note: We use the first node of the allocated resources on the cluster as the head node. However, any other node can also be chosen. <pre><code>#!/bin/bash\n#SBATCH --job-name=multi_gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --gpus-per-task=2\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\nnodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )\nnodes_array=($nodes)\nhead_node=${nodes_array[0]}\nhead_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$head_node\" hostname -I | awk '{print $1}')\nexport LOGLEVEL=INFO\n\nsrun torchrun \\\n--nnodes 1 \\\n--nproc_per_node 2 \\\n--rdzv_id $RANDOM \\\n--rdzv_backend c10d \\\n--rdzv_endpoint $head_node_ip:29501 \\\ntrain.py --compute_setup auto\n</code></pre></p>"},{"location":"tutorials/distributed/multinode_gpu/","title":"Multi Node","text":""},{"location":"tutorials/distributed/multinode_gpu/#multi-node-multi-gpu-training","title":"Multi Node Multi GPU Training","text":"<p>The following section provide Python scripts and training approach scripts for a fully distributed multi node multi gpu training.</p> <p>Some organizations use SLURM to manage resources. Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. If you are using slurm, you can use the <code>Trainer</code> by submitting a batch script using sbatch. Further below, we also provide the sbatch scripts for each setup.</p> <p>You can also use <code>torchrun</code> to run the training process - which provides a superset of the functionality as <code>torch.distributed.launch</code>. Here you need to specify the torchrun arguments arguments to set up the distributed training setup. We also include the <code>torchrun</code> sbatch scripts for each setup below.</p>"},{"location":"tutorials/distributed/multinode_gpu/#example-training-script-trainpy","title":"Example Training Script (<code>train.py</code>):","text":"<p>We are going to use the following training script for the examples below. Python Script: <pre><code>import torch\nimport argparse\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom perceptrain import TrainConfig, Trainer\nfrom perceptrain.optimize_step import optimize_step\nTrainer.set_use_grad(True)\n\n# __main__ is recommended.\nif __name__ == \"__main__\":\n    # simple dataset for y = 2\u03c0x\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    y = torch.sin(2 * torch.pi * x)\n    dataloader = DataLoader(TensorDataset(x, y), batch_size=16, shuffle=True)\n    # Simple model with no hidden layer and ReLU activation to fit the data for y = 2\u03c0x\n    model = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n    # SGD optimizer with 0.01 learning rate\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n    # TrainConfig\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--nprocs\", type=int,\n                        default=1, help=\"Number of processes (GPUs) to use.\")\n    parser.add_argument(\"--compute_setup\", type=str,\n                        default=\"auto\", choices=[\"cpu\", \"gpu\", \"auto\"], help=\"Computational Setup.\")\n    parser.add_argument(\"--backend\", type=str,\n                        default=\"nccl\", choices=[\"nccl\", \"gloo\", \"mpi\"], help=\"Distributed backend.\")\n    args = parser.parse_args()\n    train_config = TrainConfig(\n                                backend=args.backend,\n                                nprocs=args.nprocs,\n                                compute_setup=args.compute_setup,\n                                print_every=5,\n                                max_iter=50\n                            )\n\n    trainer = Trainer(model, optimizer, train_config, loss_fn=\"mse\", optimize_step=optimize_step)\n    trainer.fit(dataloader)\n</code></pre></p>"},{"location":"tutorials/distributed/multinode_gpu/#multi-node-multi-gpu","title":"Multi-Node Multi-GPU:","text":"<p>For high performance using multiple GPUs in multiple nodes. - Assuming that you have two nodes with two GPU each. These numbers can be customised on user needs.</p> <p>For multi-node, it is suggested to submit a sbatch script.</p>"},{"location":"tutorials/distributed/multinode_gpu/#slurm","title":"SLURM","text":"<ul> <li>We should have one task per gpu. i.e. <code>ntasks</code> is equal to the number of nodes.</li> <li><code>nprocs</code> should be equal to the total number of gpus (world_size). which is this case is 4.</li> </ul> <pre><code>#!/bin/bash\n#SBATCH --job-name=multi_node\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --gpus-per-task=2\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\nsrun python3 train.py --backend nccl --nprocs 4\n</code></pre>"},{"location":"tutorials/distributed/multinode_gpu/#torchrun","title":"TORCHRUN","text":"<p>Torchrun takes care of setting the <code>nprocs</code> based on the cluster setup. We only need to specify to use the <code>compute_setup</code>, which can be either <code>auto</code> or <code>gpu</code>. - <code>nnodes</code> for torchrun should be the number of nodes - <code>nproc_per_node</code> should be equal to the number of GPUs per node.</p> <p>Note: We use the first node of the allocated resources on the cluster as the head node. However, any other node can also be chosen. <pre><code>#!/bin/bash\n#SBATCH --job-name=multi_node\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --gpus-per-task=2\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\nnodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )\nnodes_array=($nodes)\nhead_node=${nodes_array[0]}\nhead_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$head_node\" hostname -I | awk '{print $1}')\nexport LOGLEVEL=INFO\n\nsrun torchrun \\\n--nnodes 2 \\\n--nproc_per_node 2 \\\n--rdzv_id $RANDOM \\\n--rdzv_backend c10d \\\n--rdzv_endpoint $head_node_ip:29501 \\\ntrain.py --compute_setup auto\n</code></pre></p>"},{"location":"tutorials/distributed/single_cpu/","title":"Single-Process CPU Training Example","text":"<p>Single-Process Training: Simple and suitable for small datasets. Use <code>backend=\"cpu\"</code>.</p> <pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom perceptrain import TrainConfig, Trainer\nfrom perceptrain.optimize_step import optimize_step\nTrainer.set_use_grad(True)\n\n# Dataset, Model, and Optimizer\nx = torch.linspace(0, 1, 100).reshape(-1, 1)\ny = torch.sin(2 * torch.pi * x)\ndataloader = DataLoader(TensorDataset(x, y), batch_size=16, shuffle=True)\nmodel = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Single-Process Training Configuration\ntrain_config = TrainConfig(compute_setup=\"cpu\", max_iter=5, print_every=1)\n\n# Training\ntrainer = Trainer(model, optimizer, train_config, loss_fn=\"mse\", optimize_step=optimize_step)\ntrainer.fit(dataloader)\n</code></pre> <pre><code>\n</code></pre> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/distributed/single_gpu/","title":"Single GPU Training","text":"<p>The following section provide Python scripts and training approach scripts for single GPU Training.</p> <p>Some organizations use SLURM to manage resources. Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. If you are using slurm, you can use the <code>Trainer</code> by submitting a batch script using sbatch. Further below, we also provide the sbatch scripts for each setup.</p> <p>You can also use <code>torchrun</code> to run the training process - which provides a superset of the functionality as <code>torch.distributed.launch</code>. Here you need to specify the torchrun arguments arguments to set up the distributed training setup. We also include the <code>torchrun</code> sbatch scripts for each setup below.</p>"},{"location":"tutorials/distributed/single_gpu/#example-training-script-trainpy","title":"Example Training Script (<code>train.py</code>):","text":"<p>We are going to use the following training script for the examples below. Python Script: <pre><code>import torch\nimport argparse\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom perceptrain import TrainConfig, Trainer\nfrom perceptrain.optimize_step import optimize_step\nTrainer.set_use_grad(True)\n\n# __main__ is recommended.\nif __name__ == \"__main__\":\n    # simple dataset for y = 2\u03c0x\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    y = torch.sin(2 * torch.pi * x)\n    dataloader = DataLoader(TensorDataset(x, y), batch_size=16, shuffle=True)\n    # Simple model with no hidden layer and ReLU activation to fit the data for y = 2\u03c0x\n    model = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n    # SGD optimizer with 0.01 learning rate\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n    # TrainConfig\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--nprocs\", type=int,\n                        default=1, help=\"Number of processes (GPUs) to use.\")\n    parser.add_argument(\"--compute_setup\", type=str,\n                        default=\"auto\", choices=[\"cpu\", \"gpu\", \"auto\"], help=\"Computational Setup.\")\n    parser.add_argument(\"--backend\", type=str,\n                        default=\"nccl\", choices=[\"nccl\", \"gloo\", \"mpi\"], help=\"Distributed backend.\")\n    args = parser.parse_args()\n    train_config = TrainConfig(\n                                backend=args.backend,\n                                nprocs=args.nprocs,\n                                compute_setup=args.compute_setup,\n                                print_every=5,\n                                max_iter=50\n                            )\n\n    trainer = Trainer(model, optimizer, train_config, loss_fn=\"mse\", optimize_step=optimize_step)\n    trainer.fit(dataloader)\n</code></pre></p>"},{"location":"tutorials/distributed/single_gpu/#single-gpu","title":"Single-GPU:","text":"<p>Simple and suitable for single-card setups. - Assuming that you have 1 node with 1 GPU.</p> <p>You can train by calling this on the head node. <pre><code>python3 train.py --backend nccl --nprocs 1\n</code></pre></p>"},{"location":"tutorials/distributed/single_gpu/#slurm","title":"SLURM","text":"<p>Slurm can be used to train to train the model. <pre><code>#!/bin/bash\n#SBATCH --job-name=single_gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --gpus-per-task=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\nsrun python3 train.py --backend nccl --nprocs 1\n</code></pre></p>"},{"location":"tutorials/distributed/single_gpu/#torchrun","title":"TORCHRUN","text":"<p>Torchrun takes care of setting the <code>nprocs</code> based on the cluster setup. We only need to specify to use the <code>compute_setup</code>, which can be either <code>auto</code> or <code>gpu</code>. - <code>nnodes</code> for torchrun should be the number of nodes - <code>nproc_per_node</code> should be equal to the number of GPUs per node.</p> <p>Note: We use the first node of the allocated resources on the cluster as the head node. However, any other node can also be chosen. <pre><code>#!/bin/bash\n#SBATCH --job-name=single_gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --gpus-per-task=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\nnodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )\nnodes_array=($nodes)\nhead_node=${nodes_array[0]}\nhead_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$head_node\" hostname -I | awk '{print $1}')\nexport LOGLEVEL=INFO\n\nsrun torchrun \\\n--nnodes 1 \\\n--nproc_per_node 1 \\\n--rdzv_id $RANDOM \\\n--rdzv_backend c10d \\\n--rdzv_endpoint $head_node_ip:29501 \\\ntrain.py --compute_setup auto\n</code></pre></p>"},{"location":"tutorials/training/examples/","title":"Examples","text":""},{"location":"tutorials/training/examples/#1-training-with-trainer-and-trainconfig","title":"1. Training with <code>Trainer</code> and <code>TrainConfig</code>","text":""},{"location":"tutorials/training/examples/#setup","title":"Setup","text":"<p>Let's do the necessary imports and declare a <code>DataLoader</code>. We can already define some hyperparameters here, including the seed for random number generators. mlflow can log hyperparameters with arbitrary types, for example the observable that we want to monitor (<code>Z</code> in this case, which has a <code>perceptrain.Operation</code> type).</p> <pre><code>import random\nfrom itertools import count\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.figure import Figure\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\n\nfrom perceptrain import hea, QuantumCircuit, Z\nfrom perceptrain.constructors import feature_map, hamiltonian_factory\nfrom perceptrain import Trainer, TrainConfig\nfrom perceptrain.data import to_dataloader\nfrom perceptrain.utils import rand_featureparameters\nfrom perceptrain import QNN, QuantumModel\nfrom perceptrain.types import ExperimentTrackingTool\n\nhyperparams = {\n    \"seed\": 42,\n    \"batch_size\": 10,\n    \"n_qubits\": 2,\n    \"ansatz_depth\": 1,\n    \"observable\": Z,\n}\n\nnp.random.seed(hyperparams[\"seed\"])\ntorch.manual_seed(hyperparams[\"seed\"])\nrandom.seed(hyperparams[\"seed\"])\n\n\ndef dataloader(batch_size: int = 25) -&gt; DataLoader:\n    x = torch.linspace(0, 1, batch_size).reshape(-1, 1)\n    y = torch.cos(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=True)\n</code></pre> <p>We continue with the regular QNN definition, together with the loss function and optimizer.</p> <pre><code>obs = hamiltonian_factory(register=hyperparams[\"n_qubits\"], detuning=hyperparams[\"observable\"])\n\ndata = dataloader(hyperparams[\"batch_size\"])\nfm = feature_map(hyperparams[\"n_qubits\"], param=\"x\")\n\nmodel = QNN(\n    QuantumCircuit(\n        hyperparams[\"n_qubits\"], fm, hea(hyperparams[\"n_qubits\"], hyperparams[\"ansatz_depth\"])\n    ),\n    observable=obs,\n    inputs=[\"x\"],\n)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ninputs = rand_featureparameters(model, 1)\n\ndef loss_fn(model: QuantumModel, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    out = model.expectation(inputs)\n    loss = criterion(out, torch.rand(1))\n    return loss, {}\n</code></pre>"},{"location":"tutorials/training/examples/#trainconfig-specifications","title":"<code>TrainConfig</code> specifications","text":"<p>perceptrain offers different tracking options via <code>TrainConfig</code>. Here we use the <code>ExperimentTrackingTool</code> type to specify that we want to track the experiment with mlflow. Tracking with tensorboard is also possible. We can then indicate what and how often we want to track or log.</p> <p>For Training <code>write_every</code> controls the number of epochs after which the loss values is logged. Thanks to the <code>plotting_functions</code> and <code>plot_every</code>arguments, we are also able to plot model-related quantities throughout training. Notice that arbitrary plotting functions can be passed, as long as the signature is the same as <code>plot_fn</code> below. Finally, the trained model can be logged by setting <code>log_model=True</code>. Here is an example of plotting function and training configuration</p> <pre><code>def plot_fn(model: Module, iteration: int) -&gt; tuple[str, Figure]:\n    descr = f\"ufa_prediction_epoch_{iteration}.png\"\n    fig, ax = plt.subplots()\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    out = model.expectation(x)\n    ax.plot(x.detach().numpy(), out.detach().numpy())\n    return descr, fig\n\n\nconfig = TrainConfig(\n    root_folder=\"mlflow_demonstration\",\n    max_iter=10,\n    checkpoint_every=1,\n    plot_every=2,\n    write_every=1,\n    log_model=True,\n    tracking_tool=ExperimentTrackingTool.MLFLOW,\n    hyperparams=hyperparams,\n    plotting_functions=(plot_fn,),\n)\n</code></pre>"},{"location":"tutorials/training/examples/#training-and-inspecting","title":"Training and inspecting","text":"<p>Model training happens as usual <pre><code>trainer = Trainer(model, optimizer, config, loss_fn)\ntrainer.fit(train_dataloader=data)\n</code></pre></p> <p>After training , we can inspect our experiment via the mlflow UI <pre><code>mlflow ui --port 8080 --backend-store-uri sqlite:///mlruns.db\n</code></pre> In this case, since we're running on a local server, we can access the mlflow UI by navigating to http://localhost:8080/.</p>"},{"location":"tutorials/training/examples/#2-fitting-a-function-with-a-qnn-using-trainer","title":"2. Fitting a function with a QNN using <code>Trainer</code>","text":"<p>In Quantum Machine Learning, the general consensus is to use <code>complex128</code> precision for states and operators and <code>float64</code> precision for parameters. This is also the convention which is used in <code>perceptrain</code>. However, for specific usecases, lower precision can greatly speed up training and reduce memory consumption. When using the <code>pyqtorch</code> backend, <code>perceptrain</code> offers the option to move a <code>QuantumModel</code> instance to a specific precision using the torch <code>to</code> syntax.</p> <p>Let's look at a complete example of how to use <code>Trainer</code> now. Here we perform a validation check during training and use a validation criterion that checks whether the validation loss in the current iteration has decreased compared to the lowest validation loss from all previous iterations. For demonstration, the train and the validation data are kept the same here. However, it is beneficial and encouraged to keep them distinct in practice to understand model's generalization capabilities.</p> <p>Note: This relies on the implementation of a quantum model</p> <pre><code>from pathlib import Path\nimport torch\nfrom functools import reduce\nfrom operator import add\nfrom itertools import count\nimport matplotlib.pyplot as plt\n\nfrom qadence import Parameter, QuantumCircuit, Z\nfrom qadence import hamiltonian_factory, hea, feature_map, chain\nfrom qadence import QNN\n\nfrom perceptrain import  TrainConfig, Trainer, to_dataloader\n\nTrainer.set_use_grad(True)\n\nn_qubits = 4\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 100\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data[0], data[1]\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n\ndef validation_criterion(\n    current_validation_loss: float, current_best_validation_loss: float, val_epsilon: float\n) -&gt; bool:\n    return current_validation_loss &lt;= current_best_validation_loss - val_epsilon\n\nn_epochs = 300\n\nconfig = TrainConfig(\n    max_iter=n_epochs,\n    batch_size=batch_size,\n    checkpoint_best_only=True,\n    val_every=10,  # The model will be run on the validation data after every `val_every` epochs.\n    validation_criterion=validation_criterion\n)\n\nfn = lambda x, degree: .05 * reduce(add, (torch.cos(i*x) + torch.sin(i*x) for i in range(degree)), 0.)\nx = torch.linspace(0, 10, batch_size).reshape(-1, 1)\ny = fn(x, 5)\n\ntrain_dataloader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_dataloader =  to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\ntrainer = Trainer(model, optimizer, config, loss_fn=loss_fn,\n                    train_dataloader=train_dataloader, val_dataloader=val_dataloader)\ntrainer.fit()\n\nplt.clf()\nplt.plot(x.numpy(), y.numpy(), label='truth')\nplt.plot(x.numpy(), model(x).detach().numpy(), \"--\", label=\"final\", linewidth=3)\nplt.legend()\nfrom docs import docsutils # markdown-exec: hide\nprint(docsutils.fig_to_html(plt.gcf())) # markdown-exec: hide\n</code></pre>"},{"location":"tutorials/training/examples/#3-performing-pre-training-exploratory-landscape-analysis-ela-with-information-content-ic","title":"3. Performing pre-training Exploratory Landscape Analysis (ELA) with Information Content (IC)","text":"<p>Before one embarks on training a model, one may wish to analyze the loss landscape to judge the trainability and catch vanishing gradient issues early. One way of doing this is made possible via calculating the Information Content of the loss landscape. This is done by discretizing the gradient in the loss landscapes and then calculating the information content therein. This serves as a measure of flatness or ruggedness of the loss landscape. Quantitatively, the information content allows us to get bounds on the average norm of the gradient in the loss landscape.</p> <p>Using the information content technique, we can get two types of bounds on the average of the norm of the gradient. 1. The bounds as achieved in the maximum Information Content regime: Gives us a lower and upper bound on the average norm of the gradient in case high Information Content is achieved. 2. The bounds as achieved in the sensitivity regime: Gives us an upper bound on the average norm of the gradient corresponding to the sensitivity IC achieved.</p> <p>Thus, we get 3 bounds. The upper and lower bounds for the maximum IC and the upper bound for the sensitivity IC.</p> <p>The <code>Trainer</code> class provides a method to calculate these gradient norms.</p> <p>Note: This relies on the implementation of a quantum model</p> <pre><code>import torch\nfrom torch.optim.adam import Adam\n\nfrom perceptrain.constructors import ObservableConfig\nfrom perceptrain.config import AnsatzConfig, FeatureMapConfig, TrainConfig\nfrom perceptrain.data import to_dataloader\nfrom perceptrain import QNN\nfrom perceptrain.optimize_step import optimize_step\nfrom perceptrain.trainer import Trainer\nfrom perceptrain.operations.primitive import Z\n\nfm_config = FeatureMapConfig(num_features=1)\nansatz_config = AnsatzConfig(depth=4)\nobs_config = ObservableConfig(detuning=Z)\n\nqnn = QNN.from_configs(\n    register=4,\n    obs_config=obs_config,\n    fm_config=fm_config,\n    ansatz_config=ansatz_config,\n)\n\noptimizer = Adam(qnn.parameters(), lr=0.001)\n\nbatch_size = 25\nx = torch.linspace(0, 1, 32).reshape(-1, 1)\ny = torch.sin(x)\ntrain_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\ntrain_config = TrainConfig(max_iter=100)\n\ntrainer = Trainer(\n    model=qnn,\n    optimizer=optimizer,\n    config=train_config,\n    loss_fn=\"mse\",\n    train_dataloader=train_loader,\n    optimize_step=optimize_step,\n)\n\n# Perform exploratory landscape analysis with Information Content\nic_sensitivity_threshold = 1e-4\nepsilons = torch.logspace(-2, 2, 10)\n\nmax_ic_lower_bound, max_ic_upper_bound, sensitivity_ic_upper_bound = (\n    trainer.get_ic_grad_bounds(\n        eta=ic_sensitivity_threshold,\n        epsilons=epsilons,\n    )\n)\n\nprint(\n    f\"Using maximum IC, the gradients are bound between {max_ic_lower_bound:.3f} and {max_ic_upper_bound:.3f}\\n\"\n)\nprint(\n    f\"Using sensitivity IC, the gradients are bounded above by {sensitivity_ic_upper_bound:.3f}\"\n)\n\n# Resume training as usual...\n\ntrainer.fit(train_loader)\n</code></pre> <p>The <code>get_ic_grad_bounds</code> function returns a tuple containing a tuple containing the lower bound as achieved in maximum IC case, upper bound as achieved in maximum IC case, and the upper bound for the sensitivity IC case.</p> <p>The sensitivity IC bound is guaranteed to appear, while the usually much tighter bounds that we get via the maximum IC case is only meaningful in the case of the maximum achieved information content \\(H(\\epsilon)_{max} \\geq log_6(2)\\).</p>"},{"location":"tutorials/training/examples/#4-custom-train-loop","title":"4. Custom <code>train</code> loop","text":"<p>If you need custom training functionality that goes beyond what is available in <code>perceptrain.Trainer</code> you can write your own training loop based on the building blocks that are available in perceptrain.</p> <p>A simplified version of perceptrain's train loop is defined below. Feel free to copy it and modify at will.</p> <p>For logging we can use the <code>get_writer</code> from the <code>Writer Registry</code>. This will set up the default writer based on the experiment tracking tool. All writers from the <code>Writer Registry</code> offer <code>open</code>, <code>close</code>, <code>print_metrics</code>, <code>write_metrics</code>, <code>plot_metrics</code>, etc methods.</p> <pre><code>from typing import Callable, Union\n\nfrom torch.nn import Module\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom perceptrain.config import TrainConfig\nfrom perceptrain.data import DictDataLoader, data_to_device\nfrom perceptrain.optimize_step import optimize_step\nfrom perceptrain.callbacks import get_writer\nfrom perceptrain.callbacks.saveload import load_checkpoint, write_checkpoint\n\n\ndef train(\n    model: Module,\n    data: DataLoader,\n    optimizer: Optimizer,\n    config: TrainConfig,\n    loss_fn: Callable,\n    device: str = \"cpu\",\n    optimize_step: Callable = optimize_step,\n    write_tensorboard: Callable = write_tensorboard,\n) -&gt; tuple[Module, Optimizer]:\n\n    # Move model to device before optimizer is loaded\n    model = model.to(device)\n\n    # load available checkpoint\n    init_iter = 0\n    if config.log_folder:\n        model, optimizer, init_iter = load_checkpoint(config.log_folder, model, optimizer)\n\n    # Initialize writer based on the tracking tool specified in the configuration\n    writer = get_writer(config.tracking_tool)  # Uses ExperimentTrackingTool to select writer\n    writer.open(config, iteration=init_iter)\n\n    dl_iter = iter(dataloader)\n\n    # outer epoch loop\n    for iteration in range(init_iter, init_iter + config.max_iter):\n        data = data_to_device(next(dl_iter), device)\n        loss, metrics = optimize_step(model, optimizer, loss_fn, data)\n\n        if iteration % config.print_every == 0 and config.verbose:\n            writer.print_metrics(OptimizeResult(iteration, model, optimizer, loss, metrics))\n\n        if iteration % config.write_every == 0:\n            writer.write(iteration, metrics)\n\n        if config.log_folder:\n            if iteration % config.checkpoint_every == 0:\n                write_checkpoint(config.log_folder, model, optimizer, iteration)\n\n    # Final writing and checkpointing\n    if config.log_folder:\n        write_checkpoint(config.log_folder, model, optimizer, iteration)\n    writer.write(iteration,metrics)\n    writer.close()\n\n    return model, optimizer\n</code></pre>"},{"location":"tutorials/training/gradient/","title":"Gradient-Based and Gradient-Free Optimization","text":"<p>The <code>Trainer</code> supports both gradient-based and gradient-free optimization methods. Default is gradient-based optimization.</p>"},{"location":"tutorials/training/gradient/#using-context-managers-for-mixed-optimization","title":"Using Context Managers for Mixed Optimization","text":"<p>For cases requiring both optimization methods in a single training session, the <code>Trainer</code> class provides context managers to enable or disable gradients.</p> <pre><code># Temporarily switch to gradient-based optimization\nwith trainer.enable_grad_opt(optimizer):\n    print(\"Gradient Based Optimization\")\n    # trainer.fit(train_loader)\n\n# Switch to gradient-free optimization for specific steps\nwith trainer.disable_grad_opt(ng_optimizer):\n    print(\"Gradient Free Optimization\")\n    # trainer.fit(train_loader)\n</code></pre>"},{"location":"tutorials/training/gradient/#using-set_grad-for-optimization-type","title":"Using set_grad for optimization type","text":"<p>We can achieve gradient free optimization with <code>Trainer.set_use_grad(False)</code> or <code>trainer.disable_grad_opt(ng_optimizer)</code>. For example, while solving a problem using gradient free optimization based on <code>Nevergrad</code> optimizers and <code>Trainer</code>.</p> <ul> <li>Gradient-Based Optimization: Utilizes optimizers from PyTorch's <code>torch.optim</code> module. This is the default behaviour of the <code>Trainer</code>, thus setting this is not necessary. However, it can be explicity mentioned as follows. Example of using gradient-based optimization:</li> </ul> <pre><code>from perceptrain import Trainer\n\n# set_use_grad(True) to enable gradient based training. This is the default behaviour of Trainer.\nTrainer.set_use_grad(True)\n</code></pre> <ul> <li>Gradient-Free Optimization: Employs optimization algorithms from the Nevergrad library.</li> </ul> <p>Example of using gradient-free optimization with Nevergrad:</p> <pre><code>from perceptrain import Trainer\n\n# set_use_grad(False) to disable gradient based training.\nTrainer.set_use_grad(False)\n</code></pre>"},{"location":"tutorials/training/hooks/","title":"Hooks for Custom Behavior","text":"<p>The <code>Trainer</code> class provides several hooks that enable users to inject custom behavior at different stages of the training process. These hooks are methods that can be overridden in a subclass to execute custom code. The available hooks include:</p> <ul> <li><code>on_train_start</code>: Called at the beginning of the training process.</li> <li><code>on_train_end</code>: Called at the end of the training process.</li> <li><code>on_train_epoch_start</code>: Called at the start of each training epoch.</li> <li><code>on_train_epoch_end</code>: Called at the end of each training epoch.</li> <li><code>on_train_batch_start</code>: Called at the start of each training batch.</li> <li><code>on_train_batch_end</code>: Called at the end of each training batch.</li> </ul> <p>Each \"start\" and \"end\" hook receives data and loss metrics as arguments. The specific values provided for these arguments depend on the training stage associated with the hook. The context of the training stage (e.g., training, validation, or testing) determines which metrics are relevant and how they are populated. For details of inputs on each hook, please review the documentation of <code>BaseTrainer</code>.</p> <pre><code>- Example of what inputs are provided to training hooks.\n\n    ```\n    def on_train_batch_start(self, batch: Tuple[torch.Tensor, ...] | None) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch.\n\n        Args:\n            batch: A batch of data from the DataLoader. Typically a tuple containing\n                input tensors and corresponding target tensors.\n        \"\"\"\n        pass\n    ```\n    ```\n    def on_train_batch_end(self, train_batch_loss_metrics: Tuple[torch.Tensor, Any]) -&gt; None:\n        \"\"\"\n        Called at the end of each training batch.\n\n        Args:\n            train_batch_loss_metrics: Metrics for the training batch loss.\n                Tuple of (loss, metrics)\n        \"\"\"\n        pass\n    ```\n</code></pre> <p>Example of using a hook to log a message at the end of each epoch:</p> <pre><code>from perceptrain import Trainer\n\nclass CustomTrainer(Trainer):\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        print(f\"End of epoch - Loss and Metrics: {train_epoch_loss_metrics}\")\n</code></pre> <p>Notes: Trainer offers inbuilt callbacks as well. Callbacks are mainly for logging/tracking purposes, but the above mentioned hooks are generic. The workflow for every train batch looks like: 1. perform on_train_batch_start callbacks, 2. call the on_train_batch_start hook, 3. do the batch training, 4. call the on_train_batch_end hook, and 5. perform on_train_batch_end callbacks.</p> <p>The use of <code>on_</code>{phase}<code>_start</code> and <code>on_</code>{phase}<code>_end</code> hooks is not specifically to add extra callbacks, but for any other generic pre/post processing. For example, reshaping input batch in case of RNNs/LSTMs, post processing loss and adding an extra metric. They could also be used to add more callbacks (which is not recommended - as we provide methods to add extra callbacks in the TrainCofig)</p>"},{"location":"tutorials/training/intro/","title":"Perceptrain Trainer Guide","text":"<p>The <code>Trainer</code> class in <code>perceptrain</code> is a versatile tool designed to streamline the training of quantum machine learning models. It offers flexibility for both gradient-based and gradient-free optimization methods, supports custom loss functions, and integrates seamlessly with tracking tools like TensorBoard and MLflow. Additionally, it provides hooks for implementing custom behaviors during the training process.</p>"},{"location":"tutorials/training/intro/#overview","title":"Overview","text":"<p>The <code>Trainer</code> class simplifies the training workflow by managing the training loop, handling data loading, and facilitating model evaluation. It is compatible with various optimization strategies and allows for extensive customization to meet specific training requirements.</p> <p>Example of initializing the <code>Trainer</code>:</p> <pre><code>from perceptrain import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\n# Initialize Trainer with model, optimizer, and configuration\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\n</code></pre>"},{"location":"tutorials/training/loss/","title":"Loss Functions","text":"<p>Users can define custom loss functions tailored to their specific tasks. The <code>Trainer</code> accepts a <code>loss_fn</code> parameter, which should be a callable that takes the model and data as inputs and returns a tuple containing the loss tensor and a dictionary of metrics.</p> <p>Example of using a custom loss function:</p> <pre><code>import torch\nfrom itertools import count\ncnt = count()\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n</code></pre> <p>This custom loss function can be used in the trainer <pre><code>from perceptrain import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\ntrainer = Trainer(model=model, optimizer=optimizer, config=config, loss_fn=loss_fn)\n</code></pre></p>"},{"location":"tutorials/training/tracking/","title":"Experiment Tracking with TensorBoard and MLflow","text":"<p>The <code>Trainer</code> integrates with TensorBoard and MLflow for experiment tracking:</p> <ul> <li> <p>TensorBoard: Logs metrics and visualizations during training, allowing users to monitor the training process.</p> </li> <li> <p>MLflow: Tracks experiments, logs parameters, metrics, and artifacts, and provides a user-friendly interface for comparing different runs.</p> </li> </ul> <p>To utilize these tracking tools, the <code>Trainer</code> can be configured with appropriate writers that handle the logging of metrics and other relevant information during training.</p> <p>Example of using TensorBoard tracking:</p> <pre><code>from perceptrain import TrainConfig\nfrom perceptrain.types import ExperimentTrackingTool\n\n# Set up tracking with TensorBoard\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.TENSORBOARD)\n</code></pre> <p>Example of using MLflow tracking:</p> <pre><code>from perceptrain.types import ExperimentTrackingTool\n\n# Set up tracking with MLflow\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.MLFLOW)\n</code></pre>"}]}